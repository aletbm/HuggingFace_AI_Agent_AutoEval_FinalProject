{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aletbm/HuggingFace_AI_Agent_AutoEval_FinalProject/blob/main/HF_Final_Project_AI_Agent_Alexander_Daniel_Rios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Agent Course - Final Project - By [Alexander Daniel Rios](https://linktr.ee/aletbm)\n",
        "<img src=\"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/thumbnail.jpg\">"
      ],
      "metadata": {
        "id": "EeD4I-UIGc5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the HF Space"
      ],
      "metadata": {
        "id": "CC3Fn8n2_56J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQr6Xtjg3JpH",
        "outputId": "a0376996-4882-4bc4-9f0d-72e62518d212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI_Agent_AutoEval'...\n",
            "remote: Enumerating objects: 247, done.\u001b[K\n",
            "remote: Counting objects: 100% (132/132), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 247 (delta 75), reused 0 (delta 0), pack-reused 115 (from 1)\u001b[K\n",
            "Receiving objects: 100% (247/247), 208.55 KiB | 5.49 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/spaces/AleTBM/AI_Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements"
      ],
      "metadata": {
        "id": "NYq8fdANAAp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfslMwHI3t39",
        "outputId": "02d38d61-b721-47b5-94da-7c69c3d96d44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/AI_Agent_AutoEval\n"
          ]
        }
      ],
      "source": [
        "%cd AI_Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9DxdqPBNNht",
        "outputId": "3fac45dd-6554-4138-b9b2-f266cfbe0f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "gradio\n",
        "requests\n",
        "langgraph\n",
        "langchain\n",
        "langchain-huggingface\n",
        "langchain-community\n",
        "langchain-experimental\n",
        "langchain-litellm\n",
        "litellm==1.67.0\n",
        "unstructured[pdf]\n",
        "duckduckgo-search\n",
        "playwright\n",
        "openai-whisper\n",
        "wikipedia\n",
        "arxiv\n",
        "yt_dlp\n",
        "pytesseract\n",
        "pyld\n",
        "docx2txt\n",
        "openpyxl\n",
        "pymupdf\n",
        "pdfplumber\n",
        "biopython\n",
        "qwen-vl-utils[decord]==0.0.8\n",
        "python-pptx\n",
        "ratelimit\n",
        "tenacity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZ3FryUm3fwW"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log in HF"
      ],
      "metadata": {
        "id": "7mHIfXgrAEk7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPYRuKux9FRZ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment variables"
      ],
      "metadata": {
        "id": "fKg-YjFi_mJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xyFDUW4-Vkk"
      },
      "outputs": [],
      "source": [
        "with open(\".env\", \"w\") as env:\n",
        "    env.write(f\"SPACE_HOST={userdata.get('SPACE_HOST')}\\n\")\n",
        "    env.write(f\"SPACE_ID={userdata.get('SPACE_ID')}\\n\")\n",
        "    env.write(f\"GEMINI_API_KEY_1={userdata.get('GOOGLE_API_KEY_1')}\\n\")\n",
        "    env.write(f\"GEMINI_API_KEY_2={userdata.get('GOOGLE_API_KEY_2')}\\n\")\n",
        "    env.write(f\"GEMINI_API_KEY_3={userdata.get('GOOGLE_API_KEY_3')}\\n\")\n",
        "    env.write(f\"USER_AGENT=MyAgent\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading GAIA files for validation and test"
      ],
      "metadata": {
        "id": "xKMxtS3BzLcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUlI3C2epQ0J",
        "outputId": "d9d480d3-7542-448b-8dad-95660e8408d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "gaia_files = \"/content/drive/MyDrive/GAIA/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "zhNuQmUxz_Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile tools.py\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.tools import Tool, StructuredTool, tool\n",
        "from typing import Tuple, List\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_community.tools import DuckDuckGoSearchResults, DuckDuckGoSearchRun\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_community.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit\n",
        "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
        "from langchain_community.document_loaders.wikipedia import WikipediaLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader\n",
        "from langchain_community.document_loaders.arxiv import ArxivLoader\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
        "import yt_dlp\n",
        "import json\n",
        "from Bio.PDB import PDBParser\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "import math\n",
        "import pdfplumber\n",
        "from pptx import Presentation\n",
        "import zipfile\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
        "from duckduckgo_search.exceptions import DuckDuckGoSearchException\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import whisper\n",
        "\n",
        "model_whisper = whisper.load_model(\"base\")\n",
        "model_image = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "processor_image = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "\n",
        "@tool\n",
        "def add(a: str, b: str) -> str:\n",
        "    \"\"\"Adds two numbers provided as strings and returns the result as a string.\"\"\"\n",
        "    return str(float(a) + float(b))\n",
        "\n",
        "@tool\n",
        "def subtract(a: str, b: str) -> str:\n",
        "    \"\"\"Subtracts the second number from the first, both provided as strings, and returns the result as a string.\"\"\"\n",
        "    return str(float(a) - float(b))\n",
        "\n",
        "@tool\n",
        "def multiply(a: str, b: str) -> str:\n",
        "    \"\"\"Multiplies two numbers provided as strings and returns the result as a string.\"\"\"\n",
        "    return str(float(a) * float(b))\n",
        "\n",
        "@tool\n",
        "def divide(a: str, b: str) -> str:\n",
        "    \"\"\"Divides the first number by the second, both provided as strings, and returns the result as a string. The divisor must not be zero.\"\"\"\n",
        "    return str(float(a) / float(b))\n",
        "\n",
        "@tool\n",
        "def power(a: str, b: str) -> str:\n",
        "    \"\"\"Raises the first number (base) to the power of the second number (exponent), both provided as strings, and returns the result as a string.\"\"\"\n",
        "    return str(float(a) ** float(b))\n",
        "\n",
        "@tool\n",
        "def square_root(a: str) -> str:\n",
        "    \"\"\"Calculates the square root of a number provided as a string and returns the result as a string. The number must be non-negative.\"\"\"\n",
        "    return str(math.sqrt(float(a)))\n",
        "\n",
        "@tool\n",
        "def get_information_from_wikipedia(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Search for relevant Wikipedia pages based on a user query and return their URLs.\n",
        "\n",
        "    This tool uses WikipediaLoader to retrieve up to 10 Wikipedia articles related to the input query.\n",
        "    The output consists of a formatted list of URLs pointing to these articles. These URLs can be used\n",
        "    by web automation tools or agents to navigate, scrape, and extract valuable information such as\n",
        "    text, tables, infoboxes, images, and references that may help answer the user's question.\n",
        "\n",
        "    Args:\n",
        "        query (str): A user-provided search query.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted explanation with a list of Wikipedia URLs related to the query.\n",
        "    \"\"\"\n",
        "    search_docs = WikipediaLoader(query=query, load_max_docs=10).load()\n",
        "    urls = [doc.metadata[\"source\"] for doc in search_docs]\n",
        "    url_list = \"\\n\".join(f\"- {url}\" for url in urls)\n",
        "\n",
        "    return f\"\"\"Wikipedia search completed for query: \"{query}\".\n",
        "\n",
        "You must now browse the following Wikipedia pages to extract detailed information. Use `navigate_browser` to explore each URL:\n",
        "\n",
        "Relevant Wikipedia URLs:\n",
        "{url_list}\n",
        "\n",
        "Instructions:\n",
        "- Visit each URL using `navigate_browser`.\n",
        "- Extract full page content, infoboxes, tables, and references.\n",
        "- Use hyperlinks from the page to recursively follow relevant internal links if necessary.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_arxiv(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the arXiv database for academic papers related to a given query and returns up to 3 relevant results,\n",
        "    including source metadata and a preview of each paper's content.\n",
        "\n",
        "    This function queries the arXiv database for papers matching the provided search term. It retrieves up to 3 papers\n",
        "    with content limited to 10,000 characters each. The results are formatted in XML-like structure with metadata\n",
        "    such as the title, authors, and page number (if available).\n",
        "\n",
        "    Args:\n",
        "        query (str): The search term or topic to query on arXiv (e.g., \"machine learning\").\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string containing the metadata (title, authors, page number) and content preview\n",
        "             for each of the top 3 relevant papers found on arXiv.\n",
        "    \"\"\"\n",
        "    search_docs = ArxivLoader(query=query, load_max_docs=3, doc_content_chars_max=10000).load()\n",
        "    content = \"\"\n",
        "    for doc in search_docs:\n",
        "        content += f\"Title: {doc.metadata['Title']}\\n\"\n",
        "        content += f\"Authors: {doc.metadata['Authors']}\\n\"\n",
        "        content += f\"Summary: {doc.metadata['Summary']}\\n\"\n",
        "        content += f\"Published: {doc.metadata['Published']}\\n\\n\"\n",
        "\n",
        "    return f\"\"\"Arxiv search completed for query: \"{query}\".\n",
        "\n",
        "Relevant Arxiv papers:\n",
        "{content}\n",
        "\n",
        "Instructions:\n",
        "- Search the paper link throught it's title.\n",
        "- Visit each URL using `navigate_browser`.\n",
        "- Extract full page content, infoboxes, tables, and references.\n",
        "- Use hyperlinks from the page to recursively follow relevant internal links if necessary.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_web_page(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Use this to extract text from basic HTML pages. Works well for static websites without JavaScript.\n",
        "\n",
        "    This function retrieves the content of a webpage at the specified URL and extracts all the visible text.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the public webpage to fetch (e.g., \"https://example.com\").\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the visible text content extracted from the webpage.\n",
        "\n",
        "    Example:\n",
        "        url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
        "        page_text = get_web_page(url)\n",
        "        print(page_text)\n",
        "\n",
        "    In this example, the function will return the visible text from the Wikipedia page about \"Python programming language\",\n",
        "    excluding JavaScript-generated or hidden content, formatted as plain text.\n",
        "    \"\"\"\n",
        "    loader = WebBaseLoader(url)\n",
        "    docs = loader.load()\n",
        "    content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    return f\"\"\"The web page content is:\n",
        "\n",
        "{content}\n",
        "\n",
        "Instructions:\n",
        "If the information is not sufficient:\n",
        "- Visit the URL {url} using `navigate_browser`.\n",
        "- Extract full page content, infoboxes, tables, and references.\n",
        "- Use hyperlinks from the page to recursively follow relevant internal links if necessary.\"\"\"\n",
        "\n",
        "def download_video_from_youtube(url, tempdirname):\n",
        "    \"\"\"\n",
        "    Downloads the best quality video from YouTube (MP4 format) and saves it in the specified directory.\n",
        "\n",
        "    This function fetches the video from the provided YouTube URL, ensuring that the video format is MP4 with a\n",
        "    resolution of at least 1080p. It also downloads subtitles if available, saving them in English as SRT files.\n",
        "\n",
        "    Args:\n",
        "        url (str): The YouTube video URL to download (e.g., \"https://www.youtube.com/watch?v=example\").\n",
        "        tempdirname (str): The directory where the downloaded video and subtitles will be saved.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the downloaded video file (MP4 format).\n",
        "\n",
        "    Example:\n",
        "        url = \"https://www.youtube.com/watch?v=example\"\n",
        "        tempdirname = \"/tmp/videos\"\n",
        "        video_filepath = download_video_from_youtube(url, tempdirname)\n",
        "        print(video_filepath)\n",
        "\n",
        "    In this example, the function will download the best quality video and subtitles from the given YouTube URL\n",
        "    and save them in the specified directory.\n",
        "    \"\"\"\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': '(bestvideo[width>=1080][ext=mp4]/bestvideo)+bestaudio/best', #Ensures best settings\n",
        "        'writesubtitles': True, #Adds a subtitles file if it exists\n",
        "        'writeautomaticsub': True, #Adds auto-generated subtitles file\n",
        "        'subtitle': '--write-sub --sub-lang en', #writes subtitles file in english\n",
        "        'subtitlesformat':'srt', #writes the subtitles file in \"srt\" or \"ass/srt/best\"\n",
        "        'skip_download': False, #skips downloading the video file\n",
        "        \"merge_output_format\": \"mp4\",\n",
        "        'outtmpl':f\"{tempdirname}/%(title)s.%(ext)s\",\n",
        "        'quiet': True,\n",
        "        'cookiefile': \"./cookies.txt\"\n",
        "        }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(url, download=True)\n",
        "        video_title = info_dict.get('title', None)\n",
        "        filepath = ydl.prepare_filename(info_dict)\n",
        "    print(f\"Download {filepath.split('/')[-1]} Successful!\")\n",
        "    return filepath\n",
        "\n",
        "def download_audio_from_youtube(url, tempdirname):\n",
        "    \"\"\"\n",
        "    Downloads the best quality audio (MP3 format) from a YouTube video and saves it in the specified directory.\n",
        "\n",
        "    This function fetches the audio from the provided YouTube URL in the best available format and saves it\n",
        "    as an MP3 file. It does not download the video.\n",
        "\n",
        "    Args:\n",
        "        url (str): The YouTube video URL to extract audio from (e.g., \"https://www.youtube.com/watch?v=example\").\n",
        "        tempdirname (str): The directory where the audio file will be saved.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the downloaded audio file (MP3 format).\n",
        "\n",
        "    Example:\n",
        "        url = \"https://www.youtube.com/watch?v=example\"\n",
        "        tempdirname = \"/tmp/audio\"\n",
        "        audio_filepath = download_audio_from_youtube(url, tempdirname)\n",
        "        print(audio_filepath)\n",
        "\n",
        "    In this example, the function will download the best quality audio (MP3) from the YouTube URL\n",
        "    and save it in the specified directory.\n",
        "    \"\"\"\n",
        "\n",
        "    ydl_opts = {\n",
        "        'extract_audio': True,\n",
        "        'format': 'bestaudio',\n",
        "        'outtmpl': f\"{tempdirname}/%(title)s.mp3\",\n",
        "        'quiet': True,\n",
        "        'cookiefile': \"./cookies.txt\"\n",
        "        }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(url, download=True)\n",
        "        audio_title = info_dict.get('title', None)\n",
        "        filepath = ydl.prepare_filename(info_dict)\n",
        "    print(f\"Download {filepath.split('/')[-1]} Successful!\")\n",
        "    return filepath\n",
        "\n",
        "def download_youtube_data(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads the description and comments of a YouTube video using the yt-dlp Python API.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        str: File paths and summary of downloaded content.\n",
        "    \"\"\"\n",
        "    ydl_opts = {\n",
        "        'quiet': True,\n",
        "        'extract_flat': False,\n",
        "        'skip_download': True,\n",
        "        'getcomments': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=False)\n",
        "\n",
        "        # Guardar descripción\n",
        "        description = info.get(\"description\", \"No description found.\")\n",
        "\n",
        "        # Guardar comentarios\n",
        "        comments = info.get(\"comments\", [])\n",
        "        comment_texts = [c[\"text\"] for c in comments if \"text\" in c]\n",
        "\n",
        "        return (\n",
        "            f\"Downloaded video data:\\n\\n\"\n",
        "            f\"Description: {description}\\n\"\n",
        "            f\"Comments: {comment_texts}\\n\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting video data: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_youtube(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads a YouTube video, extracts the audio, and returns a transcription of the audio.\n",
        "\n",
        "    This function first downloads the video and audio from the given YouTube URL. Then, it transcribes the\n",
        "    speech from the audio file using a transcription service.\n",
        "\n",
        "    Args:\n",
        "        url (str): The YouTube video URL to download and transcribe (e.g., \"https://www.youtube.com/watch?v=example\").\n",
        "\n",
        "    Returns:\n",
        "        str: The transcription text of the audio content extracted from the YouTube video.\n",
        "\n",
        "    Example:\n",
        "        url = \"https://www.youtube.com/watch?v=example\"\n",
        "        transcription = get_information_from_youtube(url)\n",
        "        print(transcription)\n",
        "\n",
        "    In this example, the function will download the video and audio from YouTube, extract the audio, and\n",
        "    return a transcription of the spoken content in the video.\n",
        "    \"\"\"\n",
        "\n",
        "    tempdirname = tempfile.TemporaryDirectory()\n",
        "    video_filepath = download_video_from_youtube(url, tempdirname)\n",
        "    audio_filepath = download_audio_from_youtube(url, tempdirname)\n",
        "    youtube_data = download_youtube_data(url)\n",
        "\n",
        "    return youtube_data + get_information_from_audio.invoke(audio_filepath)\n",
        "\n",
        "@tool\n",
        "def get_information_from_audio(filepath: str) -> str:\n",
        "    \"\"\"\n",
        "    Transcribes speech from an audio file into text using a speech-to-text model.\n",
        "\n",
        "    This function takes an audio file and converts the spoken content into written text using a speech-to-text model.\n",
        "    It is useful when you need to extract information from voice recordings.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the audio file (e.g., \".wav\", \".mp3\", etc.).\n",
        "                         The file should contain spoken content to be transcribed.\n",
        "\n",
        "    Returns:\n",
        "        str: The transcribed text from the audio file.\n",
        "\n",
        "    Example:\n",
        "        filepath = \"/path/to/audiofile.wav\"\n",
        "        transcription = get_information_from_audio(filepath)\n",
        "        print(transcription)\n",
        "\n",
        "    In this example, the function will transcribe the speech from the specified audio file and return the\n",
        "    transcribed text that can be further analyzed or used in other processes.\n",
        "    \"\"\"\n",
        "    result = model_whisper.transcribe(filepath)\n",
        "    return f\"\"\"Transcription of audio file:\n",
        "\n",
        "{result['text']}\n",
        "\n",
        "Please verify this information by cross-checking with reliable external sources such as web searches, Wikipedia, or other knowledge bases before finalizing your response.\n",
        "If necessary, supplement the transcription with additional relevant information to ensure completeness and accuracy.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts structured text content from a PDF file.\n",
        "\n",
        "    This function reads the content of a PDF document located at the specified file path and extracts the text\n",
        "    from each page. The text is returned as a single string, with the order of pages preserved.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the PDF file to be processed (e.g., \"/path/to/document.pdf\").\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing the combined text extracted from all pages of the PDF.\n",
        "             The text is ordered by the pages as they appear in the document.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"/path/to/document.pdf\"\n",
        "        extracted_text = get_information_from_pdf(file_path)\n",
        "        print(extracted_text)\n",
        "\n",
        "    In this example, the function will extract all text content from the PDF document,\n",
        "    maintaining the page order, and return it as one continuous string.\n",
        "    \"\"\"\n",
        "\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            content = page.extract_tables()\n",
        "            if content and len(content) > 0:\n",
        "                print(\"Tables were detected in the PDF.\")\n",
        "            else:\n",
        "                print(\"No tables were detected in the PDF..\")\n",
        "                loader = PyPDFLoader(file_path = file_path)\n",
        "                documents = loader.load()\n",
        "                content = '\\n'.join([doc.page_content for doc in documents])\n",
        "\n",
        "    return f\"\"\"The PDF file content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Use this data to answer the question or perform any required analysis.\n",
        "Remember, you can use all available tools to supplement with additional relevant information.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_csv(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Loads a CSV file and returns a structured summary of its tabular content.\n",
        "\n",
        "    This function reads the entire CSV file and returns a textual representation of the data\n",
        "    intended for use by AI agents or downstream systems that operate on text-based tables.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file to load.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string containing the CSV content as a table under the label `data_table`,\n",
        "             along with metadata such as the number of rows and columns.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    return f\"\"\"You are given a table extracted from a CSV file with {df.shape[0]} rows and {df.shape[1]} columns.\n",
        "\n",
        "You are given a table extracted from a CSV file.\n",
        "\n",
        "- Columns: {df.columns.tolist()}\n",
        "- Shape: {df.shape[0]} rows × {df.shape[1]} columns\n",
        "- Missing values: {df.isna().sum().sum()} total\n",
        "\n",
        "You must answer user questions using Python code with pandas-like logic. Perform all necessary computations, filtering, and aggregation using the Python interpreter tool.\n",
        "\n",
        "This dataset may contain missing values. If necessary, handle them appropriately (e.g., by filtering or imputing them) before analysis.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_excel(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts data and, when available, background color metadata from an Excel file (.xlsx or .xls).\n",
        "\n",
        "    This function handles both Excel formats:\n",
        "    - For .xlsx: extracts the full table and cell background colors.\n",
        "    - For .xls: extracts only the tabular data (background colors not supported).\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the Excel file (.xlsx or .xls).\n",
        "\n",
        "    Returns:\n",
        "        str: A detailed string containing:\n",
        "            - Table data extracted from the first sheet.\n",
        "            - If available, a matrix of background cell colors (hex format).\n",
        "            - Summary about number of rows and columns.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[-1].lower()\n",
        "\n",
        "    if ext == \".xlsx\":\n",
        "        # Load with openpyxl to extract both data and colors\n",
        "        workbook = load_workbook(file_path, data_only=True)\n",
        "        sheet = workbook.active\n",
        "        color_data = []\n",
        "\n",
        "        for row in sheet.iter_rows():\n",
        "            row_colors = []\n",
        "            for cell in row:\n",
        "                fill = cell.fill\n",
        "                color = fill.start_color.rgb if fill and fill.start_color and fill.start_color.rgb else \"None\"\n",
        "                row_colors.append(f\"#{color}\" if color != \"None\" else \"None\")\n",
        "            color_data.append(row_colors)\n",
        "\n",
        "        df = pd.read_excel(file_path, engine=\"openpyxl\", header=None)\n",
        "        return f\"\"\"You are given two tables extracted from an Excel file (.xlsx):\n",
        "\n",
        "- `data_table` contains the content of each cell.\n",
        "- `color_table` contains the background color of each cell in ARGB hex format (or \"None\" if not set).\n",
        "\n",
        "\"data_table\": {df.values.tolist()}\n",
        "\"color_table\": {color_data}\n",
        "\n",
        "Both tables are the same size: {df.shape[0]} rows × {df.shape[1]} columns.\n",
        "Use this data to analyze the spreadsheet and answer user questions.\"\"\"\n",
        "\n",
        "    elif ext == \".xls\":\n",
        "        # Load with xlrd (colors not supported)\n",
        "        df = pd.read_excel(file_path, engine=\"xlrd\", header=None)\n",
        "        return f\"\"\"You are given a table extracted from an Excel file (.xls):\n",
        "\n",
        "- Background color metadata is not available for .xls files.\n",
        "\n",
        "\"data_table\": {df.values.tolist()}\n",
        "\n",
        "The table has {df.shape[0]} rows × {df.shape[1]} columns.\n",
        "Use this data to analyze the spreadsheet and answer user questions.\"\"\"\n",
        "\n",
        "    else:\n",
        "        return \"Unsupported file format. Please provide a .xls or .xlsx Excel file.\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_xml(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads the contents of an XML file and returns it as plain text.\n",
        "\n",
        "    This function loads and parses an XML file from the specified file path and returns\n",
        "    its content as a plain text string. The text includes the structure and data contained\n",
        "    within the XML, which can help in understanding the layout and details of the document.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the XML file (e.g., \"/path/to/document.xml\").\n",
        "\n",
        "    Returns:\n",
        "        str: A plain text representation of the XML file's contents, including its structure and data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        xml_string = f.read()\n",
        "        xml_string = xml_string.replace(\">\", \">\\n\")\n",
        "        #data_dict = xmltodict.parse(xml_string)\n",
        "        #pretty_json = json.dumps(data_dict, indent=2)\n",
        "    return f\"\"\"You are given an XML document converted into JSON format:\n",
        "\n",
        "{xml_string}\n",
        "\n",
        "Use this to answer questions about the document's structure, contents, or metadata.\n",
        "If relevant, identify key entities, attributes, or relationships. \"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_json(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Loads and returns JSON content as a formatted string.\n",
        "\n",
        "    This function opens a JSON file from the specified file path, loads its contents,\n",
        "    and returns the entire JSON structure as a string. The data is formatted as valid JSON,\n",
        "    which is helpful for examining structured data in key-value pairs.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the .json file (e.g., \"/path/to/data.json\").\n",
        "\n",
        "    Returns:\n",
        "        str: A stringified version of the entire JSON data, formatted as a valid JSON string.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"/path/to/data.json\"\n",
        "        json_content = get_information_from_json(file_path)\n",
        "        print(json_content)\n",
        "\n",
        "    In this example, the function will:\n",
        "    - Open the specified JSON file.\n",
        "    - Load and parse its contents.\n",
        "    - Return the data as a JSON-formatted string that can be used for further processing or analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return f\"\"\"The content of the JSON/JSON-LD file has been successfully extracted:\n",
        "\n",
        "{json.dumps(data, indent=2)}\n",
        "\n",
        "Instruction for the agent:\n",
        "\n",
        "1. Scan the JSON content for any fields containing URLs (e.g., 'url', '@id', 'source', 'link').\n",
        "2. For each URL found, you must explore the associated website as thoroughly as possible.\n",
        "   - Do not limit the navigation to the landing page.\n",
        "   - Recursively follow internal links, sections, and dynamically loaded content.\n",
        "   - Extract valuable text, metadata, references, and any structured content.\n",
        "3. Prioritize using the tools in the following order:\n",
        "\n",
        "TOOL PRIORITY ORDER:\n",
        "1. `navigate_browser`– Use for navigating to external websites.\n",
        "2. `get_web_page` – Use for static HTML websites without JavaScript.\n",
        "3. `duckduckgo_web_search_run` – Use when a URL is missing or additional context is needed.\n",
        "\n",
        "If the JSON contains names, terms, or identifiers without direct links, consider searching Wikipedia or arXiv for background knowledge.\n",
        "\n",
        "Important:\n",
        "You must fully explore the websites found in the JSON, including internal resources and linked data. Don't stop at the home or landing page.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_pdb(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts 3D structural data from a PDB (Protein Data Bank) file.\n",
        "\n",
        "    This function parses a PDB file and generates a human-readable summary of its\n",
        "    molecular structure, including information about chains, residues, and atom positions\n",
        "    in 3D space. The output is particularly useful for understanding the structural\n",
        "    layout of proteins or other molecules stored in the PDB format.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the .pdb file (e.g., \"/path/to/structure.pdb\").\n",
        "\n",
        "    Returns:\n",
        "        str: A detailed, human-readable summary of the PDB file, listing the chains,\n",
        "             residues, and atoms with their 3D coordinates.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"/path/to/structure.pdb\"\n",
        "        pdb_info = get_information_from_pdb(file_path)\n",
        "        print(pdb_info)\n",
        "\n",
        "    In this example, the function will:\n",
        "    - Parse the PDB file located at the given path.\n",
        "    - Extract information about chains, residues, and atoms, along with their coordinates in 3D space.\n",
        "    - Return a summary of the molecular structure with the positions of atoms in x, y, and z coordinates.\n",
        "    \"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"my_protein\", file_path)\n",
        "\n",
        "    info = []\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_info = f\"Chain {chain.id} Residue {residue.resname} {residue.id[1]}:\\n\"\n",
        "                for atom in residue:\n",
        "                    coords = atom.get_coord()\n",
        "                    res_info += f\"  Atom {atom.name}: x={coords[0]:.2f}, y={coords[1]:.2f}, z={coords[2]:.2f}\\n\"\n",
        "                info.append(res_info)\n",
        "\n",
        "    return \"\\n\".join(info)\n",
        "\n",
        "@tool\n",
        "def get_information_from_txt(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts plain text content from a .txt file.\n",
        "\n",
        "    This function loads the full textual content from a local .txt file and returns it\n",
        "    as a single string. It preserves the original formatting and is useful for processing\n",
        "    documents that contain unstructured or natural language text.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Absolute or relative path to the .txt file (e.g., \"documents/file.txt\").\n",
        "\n",
        "    Returns:\n",
        "        str: The complete plain text extracted from the file.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"notes/lecture.txt\"\n",
        "        text = get_information_from_txt(file_path)\n",
        "        print(text)\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to read or analyze the full content of a text file.\n",
        "    - The input is stored in a standard plain-text format (.txt).\n",
        "    \"\"\"\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    content = \"\\n\".join([doc.page_content for doc in documents])\n",
        "    return f\"The TXT file content is:\\n\\n {content}\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_python(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Loads and returns the source code from a Python (.py) file.\n",
        "\n",
        "    This function opens a local Python script from the specified file path, reads its entire\n",
        "    source code, and returns it as a formatted string. It's useful for inspecting, analyzing,\n",
        "    or executing the contents of Python files in later steps.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Absolute or relative path to the Python file (e.g., \"scripts/my_script.py\").\n",
        "\n",
        "    Returns:\n",
        "        str: The full Python source code from the file as a string.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"models/model_utils.py\"\n",
        "        code = get_information_from_python(file_path)\n",
        "        print(code)\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to analyze or execute the contents of a Python script.\n",
        "    - The input is a .py file and contains valid Python code.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        code = f.read()\n",
        "    return f\"\"\"Python source code extracted from: {file_path}\n",
        "\n",
        "{code}\n",
        "\n",
        "Suggested next step:\n",
        "If you want to analyze or run this code, use the python_code_executor tool and pass the code directly as input.\n",
        "\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_docx(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text content from a Microsoft Word (.docx) file.\n",
        "\n",
        "    This function reads a .docx file from the specified file path and returns all\n",
        "    human-readable text as a single string, preserving the logical order of the document.\n",
        "    It is useful when you need to analyze or summarize documents created in Microsoft Word.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Full or relative path to the .docx file (e.g., \"reports/report.docx\").\n",
        "\n",
        "    Returns:\n",
        "        str: The complete extracted text from the Word document.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"contracts/agreement.docx\"\n",
        "        text = get_information_from_docx(file_path)\n",
        "        print(text)\n",
        "\n",
        "    Use this tool when:\n",
        "    - The input file is a .docx Word document.\n",
        "    - You want to extract the entire textual content for processing, querying, or summarization.\n",
        "    \"\"\"\n",
        "    loader = Docx2txtLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    return \"\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "@tool\n",
        "def get_information_from_pptx(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts all text from a PowerPoint (.pptx) file slide by slide.\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to read and analyze the text content of a presentation.\n",
        "    - You want to understand slide structure or extract meaningful information from slides.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the .pptx file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text content from the presentation.\n",
        "    \"\"\"\n",
        "    prs = Presentation(file_path)\n",
        "    all_text = []\n",
        "\n",
        "    for i, slide in enumerate(prs.slides):\n",
        "        slide_text = []\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                slide_text.append(shape.text.strip())\n",
        "        if slide_text:\n",
        "            all_text.append(f\"- Slide {i + 1}\\n\" + \"\\n\".join(slide_text))\n",
        "    all_text = '\\n\\n'.join(all_text)\n",
        "    return f\"\"\"The PPTX file content is:\n",
        "\n",
        "{all_text}\n",
        "\n",
        "You can use this information to answer the user question.\n",
        "\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_image(file_path: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs visual question answering (VQA) on an image file.\n",
        "\n",
        "    This tool allows the agent to reason about the contents of an image. It takes a path to an image file\n",
        "    and a natural language question, processes the image and text using a vision-language model,\n",
        "    and returns a text-based answer derived from the visual information.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the image file (e.g., \"images/photo.png\").\n",
        "        question (str): Natural language question to ask about the image (e.g., \"What is the person holding?\").\n",
        "\n",
        "    Returns:\n",
        "        str: Answer generated by the vision-language model based on the image and the question.\n",
        "\n",
        "    Example:\n",
        "        answer = get_information_from_image(\"cat.jpg\", \"What color is the cat?\")\n",
        "        print(answer)\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to analyze or describe visual content in an image.\n",
        "    - The user asks a question involving a photo, diagram, chart, screenshot, or other image file.\n",
        "    - Visual reasoning is required to answer the question.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": file_path,\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": question},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text = processor_image.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor_image(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    generated_ids = model_image.generate(**inputs, max_new_tokens=1024)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor_image.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return f\"VISUAL ANSWER: {output_text}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_all_files_from_zip(file_path: str) -> Tuple[List[str], str]:\n",
        "    \"\"\"\n",
        "    Extracts all files from a ZIP archive to a temporary directory and returns their paths.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the .zip archive.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], str]:\n",
        "            - A list of full paths to the extracted files.\n",
        "            - A summary string indicating how many files were extracted.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "    zip_ref.extractall(temp_dir)\n",
        "    zip_files = zip_ref.namelist()\n",
        "    for filename in zip_ref.namelist():\n",
        "        temp_file_path = os.path.join(temp_dir, filename)\n",
        "        files.append(temp_file_path)\n",
        "\n",
        "    prompt = f\"The ZIP file contains {len(files)} file(s).\"\n",
        "    return files, prompt\n",
        "\n",
        "\n",
        "duckduckgosearchrun_tool = DuckDuckGoSearchRun()\n",
        "duckduckgosearchresults_tool = DuckDuckGoSearchResults(max_results=5, output_format='list')\n",
        "\n",
        "@retry(\n",
        "    wait=wait_exponential(multiplier=1, min=15, max=60),\n",
        "    stop=stop_after_attempt(3),\n",
        "    retry=retry_if_exception_type(DuckDuckGoSearchException)\n",
        ")\n",
        "@sleep_and_retry\n",
        "@limits(calls=1, period=30)\n",
        "def get_dgg_search(query: str, mode: str):\n",
        "    if mode == \"run\":\n",
        "        return duckduckgosearchrun_tool.run(query)\n",
        "    elif mode == \"results\":\n",
        "        return duckduckgosearchresults_tool.run(query)\n",
        "\n",
        "def get_information_from_searches(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform a DuckDuckGo search and return both a textual summary and structured results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "        str: Combined summary and list of links with navigation instructions.\n",
        "    \"\"\"\n",
        "    text_summary = get_dgg_search(query, mode=\"run\")\n",
        "    structured_results = get_dgg_search(query, mode=\"results\")\n",
        "\n",
        "    formatted_links = \"\\n\".join([\n",
        "        f\"- [{res['title']}]({res['link']}):\\n{res['snippet']}\"\n",
        "        for res in structured_results])\n",
        "\n",
        "    text_summary = \"No results found.\" if len(text_summary) < 5 else text_summary\n",
        "    formatted_links = \"No results found.\" if len(formatted_links) < 5 else formatted_links\n",
        "\n",
        "    return f\"\"\"DuckDuckGo Search Results for: \"{query}\"\n",
        "\n",
        "Summary:\n",
        "{text_summary}\n",
        "\n",
        "Top Links:\n",
        "{formatted_links}\n",
        "\n",
        "Instructions for the Agent:\n",
        "If the summary is insufficient or lacks detail:\n",
        "- Use `navigate_browser` to open one or more of the above links.\n",
        "- Extract the full page content, including main text, tables, and sidebars.\n",
        "- Optionally, explore internal links within those pages if relevant to the query.\n",
        "\"\"\"\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"duckduckgo_search\",\n",
        "    func=get_information_from_searches,\n",
        "    description=(\n",
        "        \"Use this tool to perform a live DuckDuckGo web search and retrieve both a summary and a list of relevant links \"\n",
        "        \"with snippets. Ideal for finding general information from public websites that are well indexed by DuckDuckGo.\\n\\n\"\n",
        "        \"Limitations:\\n\"\n",
        "        \"- DuckDuckGo may not return results for sites like Google.\\n\"\n",
        "        \"Fallback Instructions:\\n\"\n",
        "        \"If this tool returns no useful information or fails to retrieve results:\\n\"\n",
        "        \"- Use `navigate_browser` to open relevant links or perform a direct search in a browser context.\\n\"\n",
        "        \"- Extract detailed information from the visited web pages, including main text, sidebars, and tables.\\n\"\n",
        "        \"- Explore internal links within the pages if needed to answer the question.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Python interpreter\n",
        "def verbose_python_executor(code: str) -> str:\n",
        "    \"\"\"Executes Python code and returns both the code and the output.\"\"\"\n",
        "    tool = PythonREPLTool()\n",
        "    result = tool.run(code)\n",
        "    return f\"Code executed:\\n```python\\n{code}\\n```\\n\\nOutput:\\n{result}\"\n",
        "\n",
        "python_tool = Tool(\n",
        "    name=\"python_code_executor\",\n",
        "    func=verbose_python_executor,\n",
        "    description=(\n",
        "        \"Use this tool to execute raw Python code.\\n\\n\"\n",
        "        \"Input: A Python code snippet as a single string.\\n\\n\"\n",
        "        \"Important:\\n\"\n",
        "        \"- This is NOT a remote API call. Do NOT include calls to `default_api`, `__arg1`, or any tool inside the code.\\n\"\n",
        "        \"- The code should be ready to run directly in Python, using standard libraries.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "def download_file_temp(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads a file from the given URL and saves it into a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the file to download.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded file or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Create a temporary directory\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Extract filename from URL or fallback to a generic name\n",
        "        filename = url.split(\"/\")[-1] or \"downloaded_file\"\n",
        "\n",
        "        # Full path for saving file\n",
        "        file_path = os.path.join(temp_dir, filename)\n",
        "\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        return f\"File downloaded successfully and saved at {file_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Failed to download file: {e}\"\n",
        "\n",
        "download_tool = Tool(\n",
        "    name=\"file_downloader_temp\",\n",
        "    func=download_file_temp,\n",
        "    description=\"Downloads a file from a URL and saves it in a temporary folder.\"\n",
        ")\n",
        "\n",
        "# Create a Playwright browser instance\n",
        "def initialize_web_tools():\n",
        "    async_browser = create_async_playwright_browser()\n",
        "    toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
        "    return toolkit.get_tools()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_td2rLcltSUo",
        "outputId": "5affb83b-6984-4b3a-c24b-e635d6c7e0da"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting tools.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ],
      "metadata": {
        "id": "W1JwCkBG0kCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agent.py\n",
        "\n",
        "import os\n",
        "import time\n",
        "from langchain.tools import Tool, tool\n",
        "from typing import Tuple, List\n",
        "from typing_extensions import TypedDict, Annotated, Optional\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.graph import START, StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_litellm import ChatLiteLLM\n",
        "from IPython.display import Image, display\n",
        "from tools import (search_tool,\n",
        "                   download_tool,\n",
        "                   get_web_page,\n",
        "                   add,\n",
        "                   subtract,\n",
        "                   multiply,\n",
        "                   divide,\n",
        "                   power,\n",
        "                   square_root,\n",
        "                   get_information_from_wikipedia,\n",
        "                   get_information_from_arxiv,\n",
        "                   get_information_from_youtube,\n",
        "                   python_tool,\n",
        "                   get_information_from_json,\n",
        "                   get_information_from_audio,\n",
        "                   get_information_from_xml,\n",
        "                   get_information_from_docx,\n",
        "                   get_information_from_txt,\n",
        "                   get_information_from_pdf,\n",
        "                   get_information_from_csv,\n",
        "                   get_information_from_excel,\n",
        "                   get_information_from_pdb,\n",
        "                   get_information_from_image,\n",
        "                   get_information_from_pptx,\n",
        "                   get_all_files_from_zip)\n",
        "\n",
        "DELAY = 5\n",
        "TIME_SLEEP = 60/15 + DELAY\n",
        "\n",
        "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY_1\")\n",
        "\n",
        "chat_model = ChatLiteLLM(model=\"gemini/gemini-2.0-flash\",\n",
        "                         temperature=0.1,\n",
        "                         api_key=GEMINI_API_KEY,\n",
        "                         max_retries=10,\n",
        "                         verbose=True)\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "    question: Optional[str]\n",
        "    file_path: Optional[str]\n",
        "    task_id: Optional[str]\n",
        "    new_messages: Optional[int]\n",
        "\n",
        "class MyAgent:\n",
        "    def __init__(self, web_tools=None):\n",
        "        print(\"MyAgent initialized.\")\n",
        "\n",
        "        self.chat = chat_model\n",
        "\n",
        "        self.tools = [search_tool,\n",
        "                    download_tool,\n",
        "                    get_web_page,\n",
        "                    add,\n",
        "                    subtract,\n",
        "                    multiply,\n",
        "                    divide,\n",
        "                    power,\n",
        "                    square_root,\n",
        "                    get_information_from_wikipedia,\n",
        "                    get_information_from_arxiv,\n",
        "                    get_information_from_youtube,\n",
        "                    python_tool,\n",
        "                    get_information_from_json,\n",
        "                    get_information_from_audio,\n",
        "                    get_information_from_xml,\n",
        "                    get_information_from_docx,\n",
        "                    get_information_from_txt,\n",
        "                    get_information_from_pdf,\n",
        "                    get_information_from_csv,\n",
        "                    get_information_from_excel,\n",
        "                    get_information_from_pdb,\n",
        "                    get_information_from_image,\n",
        "                    get_information_from_pptx,\n",
        "                    get_all_files_from_zip\n",
        "                    ] + web_tools\n",
        "\n",
        "        self.chat_with_tools = self.chat.bind_tools(self.tools, verbose=True)\n",
        "\n",
        "        self.builder = StateGraph(AgentState)\n",
        "        self.builder.add_node(\"assistant\", self.assistant)\n",
        "        self.builder.add_node(\"tools\", ToolNode(self.tools))\n",
        "        self.builder.add_node(\"extract_data_from_file\", self.extract_data_from_file)\n",
        "        self.builder.add_node(\"postprocess\", self.postprocess)\n",
        "\n",
        "        self.builder.add_edge(START, \"extract_data_from_file\")\n",
        "        self.builder.add_edge(\"extract_data_from_file\", \"assistant\")\n",
        "        self.builder.add_conditional_edges(\n",
        "            \"assistant\",\n",
        "            self.assistant_router,\n",
        "            {\n",
        "                \"tools\": \"tools\",\n",
        "                \"postprocess\": \"postprocess\"\n",
        "            }\n",
        "        )\n",
        "        self.builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "        self.agent = self.builder.compile()\n",
        "\n",
        "    async def __call__(self, question: str, file_path: str, task_id: str) -> str:\n",
        "        print(\"\\033[1m\\033[93m\"+\"=\"*150+\"\\033[0m\")\n",
        "        print(f\"QUESTION: {question}\")\n",
        "        print(f\"File: {file_path}\")\n",
        "        prompt = f\"\"\"You are a general AI assistant. I will ask you a question and I give you the extracted data from associate files to the user question.\n",
        "\n",
        "You must follow this process:\n",
        "1. Analyze the user question to identify the required output type (e.g., number, string, list) and key concepts.\n",
        "2. BEFORE planning or using any tool, determine if the answer can be obtained directly from your own knowledge or reasoning. You can't guest the answer.\n",
        "   - If yes, answer directly without using any tools.\n",
        "   - If not, proceed with tool-based planning as described below.\n",
        "3. If tools are needed, generate a plan that includes:\n",
        "   - The approach to solve the question.\n",
        "   - Which tools to use and in what order.\n",
        "   - How to reformulate the query if needed for web search.\n",
        "   - Ensure that search queries do not contain punctuation marks, commas, quotes, or special characters.\n",
        "4. Do not execute any tool until the plan is complete.\n",
        "5. Follow the plan exactly. If a tool fails (e.g., due to network or no results), pause, replan, and retry with:\n",
        "   - A reformulated query (more specific or with synonyms/context).\n",
        "   - A fallback tool if available.\n",
        "6. Evaluate tool results for relevance. If multiple source links are available, explore each one using `navigate_browser` to gather all relevant information.\n",
        "\n",
        "Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\n",
        "\n",
        "YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\n",
        "If you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unless specified otherwise.\n",
        "If you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\n",
        "If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\"\"\"\n",
        "\n",
        "        user_message = f\"Question: {question}\\nFilepath: {file_path}\"\n",
        "\n",
        "        messages = [SystemMessage(content=prompt, name=\"SYSTEM\"),\n",
        "                    HumanMessage(content=user_message, name=\"USER\")]\n",
        "\n",
        "        response = await self.agent.ainvoke({\"messages\": messages,\n",
        "                                             \"question\": question,\n",
        "                                             \"file_path\": file_path,\n",
        "                                             \"task_id\": task_id,\n",
        "                                             \"new_messages\": 1},\n",
        "                                              {\"recursion_limit\": 100})\n",
        "\n",
        "        print(\"\\033[1m\\033[93m\"+\"=\"*150+\"\\033[0m\")\n",
        "        return response['messages'][-1].content\n",
        "\n",
        "    async def assistant(self, state: AgentState):\n",
        "        new_messages = state[\"new_messages\"]\n",
        "\n",
        "        for i in reversed(range(1, new_messages+1)):\n",
        "            print(\"\\033[1m\\033[92m\"+\"+\"*150+\"\\033[0m\")\n",
        "            name = state[\"messages\"][-i].name\n",
        "            content = state[\"messages\"][-i].content\n",
        "            print(f'\\033[1m\\033[96m{name}\\033[0m: {content if len(content) < 5000 else content[:5000]}')\n",
        "\n",
        "        result = await self.chat_with_tools.ainvoke(state[\"messages\"])\n",
        "        result.name=\"ASSISTANT\"\n",
        "        time.sleep(TIME_SLEEP)\n",
        "\n",
        "        print(\"\\033[1m\\033[92m\"+\"+\"*150+\"\\033[0m\")\n",
        "        content = result.content[:-2] if result.content[-2:] == '\\n\\n' else result.content\n",
        "        print(f'\\033[1m\\033[96m{result.name}\\033[0m: {content}')\n",
        "        new_messages = 1\n",
        "\n",
        "        return {\"messages\": result, \"new_messages\": new_messages}\n",
        "\n",
        "    def extract_data_from_file(self, state: AgentState) -> str:\n",
        "        path = state[\"file_path\"]\n",
        "        new_messages = state[\"new_messages\"]\n",
        "        prompt = \"\"\n",
        "        messages = []\n",
        "\n",
        "        if path and \".\" in path:\n",
        "            ext = path.strip().split(\".\")[-1].lower()\n",
        "            print(f\"Extension detected: {ext}\")\n",
        "\n",
        "            if ext == \"zip\":\n",
        "                files, prompt = get_all_files_from_zip(path)\n",
        "                name = \"get_all_file_from_zip\"\n",
        "                messages.append(AIMessage(content=prompt, name=name))\n",
        "            else:\n",
        "                files = [path]\n",
        "\n",
        "            for file_path in files:\n",
        "                ext = file_path.strip().split(\".\")[-1].lower()\n",
        "                print(f\"Extension detected: {ext}\")\n",
        "\n",
        "                prompt = f\"Information extracted from {file_path}.\\n\\n\"\n",
        "                match ext:\n",
        "                    case \"csv\":\n",
        "                        content = get_information_from_csv.invoke(file_path)\n",
        "                        name = \"get_information_from_csv\"\n",
        "                    case \"txt\":\n",
        "                        content = get_information_from_txt.invoke(file_path)\n",
        "                        name = \"get_information_from_txt\"\n",
        "                    case \"pdf\":\n",
        "                        content = get_information_from_pdf.invoke(file_path)\n",
        "                        name = \"get_information_from_pdf\"\n",
        "                    case \"json\":\n",
        "                        content = get_information_from_json.invoke(file_path)\n",
        "                        name = \"get_information_from_json\"\n",
        "                    case \"jsonld\":\n",
        "                        content = get_information_from_json.invoke(file_path)\n",
        "                        name = \"get_information_from_json\"\n",
        "                    case \"xml\":\n",
        "                        content = get_information_from_xml.invoke(file_path)\n",
        "                        name = \"get_information_from_xml\"\n",
        "                    case \"pdb\":\n",
        "                        content = get_information_from_pdb.invoke(file_path)\n",
        "                        name = \"get_information_from_pdb\"\n",
        "                    case \"mp3\":\n",
        "                        content = get_information_from_audio.invoke(file_path)\n",
        "                        name = \"get_information_from_audio\"\n",
        "                    case \"m4a\":\n",
        "                        content = get_information_from_audio.invoke(file_path)\n",
        "                        name = \"get_information_from_audio\"\n",
        "                    case \"docx\":\n",
        "                        content = get_information_from_docx.invoke(file_path)\n",
        "                        name = \"get_information_from_docx\"\n",
        "                    case \"xlsx\":\n",
        "                        content = get_information_from_excel.invoke(file_path)\n",
        "                        name = \"get_information_from_excel\"\n",
        "                    case \"xls\":\n",
        "                        content = get_information_from_excel.invoke(file_path)\n",
        "                        name = \"get_information_from_excel\"\n",
        "                    case \"png\":\n",
        "                        content = get_information_from_image.invoke({\"file_path\": file_path, \"question\": state[\"question\"]})\n",
        "                        name = \"get_information_from_image\"\n",
        "                    case \"jpg\":\n",
        "                        content = get_information_from_image.invoke({\"file_path\": file_path, \"question\": state[\"question\"]})\n",
        "                        name = \"get_information_from_image\"\n",
        "                    case \"py\":\n",
        "                        content = get_information_from_python.invoke(file_path)\n",
        "                        name = \"get_information_from_python\"\n",
        "                    case \"pptx\":\n",
        "                        content = get_information_from_pptx.invoke(file_path)\n",
        "                        name = \"get_information_from_pptx\"\n",
        "                    case _:\n",
        "                        content = \"Try to use some available tool to answer the user question.\"\n",
        "                        name = \"handle_no_file\"\n",
        "                prompt += f\"{content}\"\n",
        "                messages.append(AIMessage(content=prompt, name=name))\n",
        "                new_messages += 1\n",
        "        else:\n",
        "            prompt = \"The question doesn't have an attached file.\"\n",
        "            name = \"handle_no_file\"\n",
        "\n",
        "        return {\"messages\": messages, \"new_messages\": new_messages}\n",
        "\n",
        "    def assistant_router(self, state: AgentState) -> str:\n",
        "        tool_decision = tools_condition(state)\n",
        "        if tool_decision == \"tools\":\n",
        "            return \"tools\"\n",
        "        else:\n",
        "            return \"postprocess\"\n",
        "\n",
        "    def postprocess(self, state: AgentState) -> AgentState:\n",
        "        last_msg = state[\"messages\"][-1]\n",
        "        content = last_msg.content\n",
        "        index = content.find(\"FINAL ANSWER: \")\n",
        "        if index != -1:\n",
        "            content = content[index+len(\"FINAL ANSWER: \"):].replace(\"\\n\", \"\")\n",
        "        else:\n",
        "            content = \"Unable to find the answer\"\n",
        "\n",
        "        state[\"messages\"][-1].content = content\n",
        "        return state\n",
        "\n",
        "    def draw_graph(self):\n",
        "        display(Image(self.agent.get_graph().draw_mermaid_png()))\n",
        "        return"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9mUVI9Z0sHE",
        "outputId": "402c106d-3687-4758-864c-662beb4d181c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lPdKbuIO0tm"
      },
      "source": [
        "# App"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from __future__ import unicode_literals\n",
        "import subprocess\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "subprocess.run([\"playwright\", \"install\"])\n",
        "subprocess.run([\"playwright\", \"install-deps\"])\n",
        "subprocess.run([\"playwright\", \"install\", \"chromium\"])\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "os.environ['USER_AGENT'] = 'myagent'\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import inspect\n",
        "import tempfile\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "\n",
        "from agent import MyAgent\n",
        "from tools import initialize_web_tools\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "DEFAULT_API_URL = \"https://agents-course-unit4-scoring.hf.space\"\n",
        "space_host_startup = os.getenv(\"SPACE_HOST\")\n",
        "space_id_startup = os.getenv(\"SPACE_ID\")\n",
        "\n",
        "def fetch_questions(questions_url):\n",
        "    print(f\"Fetching questions from: {questions_url}\")\n",
        "    try:\n",
        "        response = requests.get(questions_url, timeout=15)\n",
        "        response.raise_for_status()\n",
        "        questions_data = response.json()\n",
        "        if not questions_data:\n",
        "             print(\"Fetched questions list is empty.\")\n",
        "             return \"Fetched questions list is empty or invalid format.\", None\n",
        "        print(f\"Fetched {len(questions_data)} questions.\")\n",
        "        return questions_data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching questions: {e}\")\n",
        "        return f\"Error fetching questions: {e}\", None\n",
        "    except requests.exceptions.JSONDecodeError as e:\n",
        "         print(f\"Error decoding JSON response from questions endpoint: {e}\")\n",
        "         print(f\"Response text: {response.text[:500]}\")\n",
        "         return f\"Error decoding server response for questions: {e}\", None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred fetching questions: {e}\")\n",
        "        return f\"An unexpected error occurred fetching questions: {e}\", None\n",
        "\n",
        "def download_file(files_url, task_id):\n",
        "    try:\n",
        "        filename = task_id\n",
        "        response = requests.get(files_url+'/'+filename)\n",
        "        response.raise_for_status()\n",
        "        #print(f\"Downloaded file for task {task_id}\")\n",
        "\n",
        "        filename = response.headers['content-disposition'].split('\"')[1]\n",
        "        temp_dir = tempfile.gettempdir()\n",
        "        file_path = os.path.join(temp_dir, filename)\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        #print(file_path)\n",
        "\n",
        "        return file_path\n",
        "    except Exception as file_e:\n",
        "        #print(f\"No file found for task {task_id} or error: {file_e}\")\n",
        "        return None\n",
        "\n",
        "async def run_my_agent(agent, questions_data, files_url):\n",
        "    results_log = []\n",
        "    answers_payload = []\n",
        "    print(f\"Running agent on {len(questions_data)} questions...\")\n",
        "    for item in questions_data:\n",
        "        task_id = item.get(\"task_id\")\n",
        "        file_path = download_file(files_url, task_id)\n",
        "        question_text = item.get(\"question\")\n",
        "        if not task_id or question_text is None:\n",
        "            print(f\"Skipping item with missing task_id or question: {item}\")\n",
        "            continue\n",
        "        try:\n",
        "            submitted_answer = await agent(question_text, file_path, task_id)\n",
        "            answers_payload.append({\"task_id\": task_id, \"submitted_answer\": submitted_answer})\n",
        "            results_log.append({\"Task ID\": task_id, \"Question\": question_text, \"Submitted Answer\": submitted_answer})\n",
        "        except Exception as e:\n",
        "             print(f\"Error running agent on task {task_id}: {e}\")\n",
        "             results_log.append({\"Task ID\": task_id, \"Question\": question_text, \"Submitted Answer\": f\"AGENT ERROR: {e}\"})\n",
        "    return results_log, answers_payload\n",
        "\n",
        "async def run_and_submit_all( profile: gr.OAuthProfile | None):\n",
        "    \"\"\"\n",
        "    Fetches all questions, runs the BasicAgent on them, submits all answers,\n",
        "    and displays the results.\n",
        "    \"\"\"\n",
        "    # --- Determine HF Space Runtime URL and Repo URL ---\n",
        "    #space_id = os.getenv(\"SPACE_ID\") # Get the SPACE_ID for sending link to the code\n",
        "\n",
        "    if profile:\n",
        "        username= f\"{profile.username}\"\n",
        "        print(f\"User logged in: {username}\")\n",
        "    else:\n",
        "        print(\"User not logged in.\")\n",
        "        return \"Please Login to Hugging Face with the button.\", None\n",
        "\n",
        "    api_url = DEFAULT_API_URL\n",
        "    questions_url = f\"{api_url}/questions\"\n",
        "    submit_url = f\"{api_url}/submit\"\n",
        "    files_url = f\"{api_url}/files\"\n",
        "\n",
        "    # 1. Instantiate Agent ( modify this part to create your agent)\n",
        "    try:\n",
        "        web_tools = initialize_web_tools()\n",
        "        agent = MyAgent(web_tools=web_tools)\n",
        "    except Exception as e:\n",
        "        print(f\"Error instantiating agent: {e}\")\n",
        "        return f\"Error initializing agent: {e}\", None\n",
        "    # In the case of an app running as a hugging Face space, this link points toward your codebase ( usefull for others so please keep it public)\n",
        "    agent_code = f\"https://huggingface.co/spaces/{space_id_startup}/tree/main\"\n",
        "    print(agent_code)\n",
        "\n",
        "    # 2. Fetch Questions\n",
        "    questions_data = fetch_questions(questions_url)\n",
        "\n",
        "    # 3. Run your Agent\n",
        "    results_log, answers_payload = await run_my_agent(agent, questions_data, files_url)\n",
        "    #results_log, answers_payload = run_my_agent(questions_data, api_url)\n",
        "    if not answers_payload:\n",
        "        print(\"Agent did not produce any answers to submit.\")\n",
        "        return \"Agent did not produce any answers to submit.\", pd.DataFrame(results_log)\n",
        "\n",
        "    # 4. Prepare Submission\n",
        "    submission_data = {\"username\": username.strip(), \"agent_code\": agent_code, \"answers\": answers_payload}\n",
        "    status_update = f\"Agent finished. Submitting {len(answers_payload)} answers for user '{username}'...\"\n",
        "    print(status_update)\n",
        "\n",
        "    # 5. Submit\n",
        "    print(f\"Submitting {len(answers_payload)} answers to: {submit_url}\")\n",
        "    try:\n",
        "        response = requests.post(submit_url, json=submission_data, timeout=60)\n",
        "        response.raise_for_status()\n",
        "        result_data = response.json()\n",
        "        final_status = (\n",
        "            f\"Submission Successful!\\n\"\n",
        "            f\"User: {result_data.get('username')}\\n\"\n",
        "            f\"Overall Score: {result_data.get('score', 'N/A')}% \"\n",
        "            f\"({result_data.get('correct_count', '?')}/{result_data.get('total_attempted', '?')} correct)\\n\"\n",
        "            f\"Message: {result_data.get('message', 'No message received.')}\"\n",
        "        )\n",
        "        print(\"Submission successful.\")\n",
        "        results_df = pd.DataFrame(results_log)\n",
        "        return final_status, results_df\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        error_detail = f\"Server responded with status {e.response.status_code}.\"\n",
        "        try:\n",
        "            error_json = e.response.json()\n",
        "            error_detail += f\" Detail: {error_json.get('detail', e.response.text)}\"\n",
        "        except requests.exceptions.JSONDecodeError:\n",
        "            error_detail += f\" Response: {e.response.text[:500]}\"\n",
        "        status_message = f\"Submission Failed: {error_detail}\"\n",
        "        print(status_message)\n",
        "        results_df = pd.DataFrame(results_log)\n",
        "        return status_message, results_df\n",
        "    except requests.exceptions.Timeout:\n",
        "        status_message = \"Submission Failed: The request timed out.\"\n",
        "        print(status_message)\n",
        "        results_df = pd.DataFrame(results_log)\n",
        "        return status_message, results_df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        status_message = f\"Submission Failed: Network error - {e}\"\n",
        "        print(status_message)\n",
        "        results_df = pd.DataFrame(results_log)\n",
        "        return status_message, results_df\n",
        "    except Exception as e:\n",
        "        status_message = f\"An unexpected error occurred during submission: {e}\"\n",
        "        print(status_message)\n",
        "        results_df = pd.DataFrame(results_log)\n",
        "        return status_message, results_df\n",
        "\n",
        "\n",
        "# --- Build Gradio Interface using Blocks ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Basic Agent Evaluation Runner\")\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        **Instructions:**\n",
        "        1.  Please clone this space, then modify the code to define your agent's logic, the tools, the necessary packages, etc ...\n",
        "        2.  Log in to your Hugging Face account using the button below. This uses your HF username for submission.\n",
        "        3.  Click 'Run Evaluation & Submit All Answers' to fetch questions, run your agent, submit answers, and see the score.\n",
        "        ---\n",
        "        **Disclaimers:**\n",
        "        Once clicking on the \"submit button, it can take quite some time ( this is the time for the agent to go through all the questions).\n",
        "        This space provides a basic setup and is intentionally sub-optimal to encourage you to develop your own, more robust solution. For instance for the delay process of the submit button, a solution could be to cache the answers and submit in a seperate action or even to answer the questions in async.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.LoginButton()\n",
        "\n",
        "    run_button = gr.Button(\"Run Evaluation & Submit All Answers\")\n",
        "\n",
        "    status_output = gr.Textbox(label=\"Run Status / Submission Result\", lines=5, interactive=False)\n",
        "    # Removed max_rows=10 from DataFrame constructor\n",
        "    results_table = gr.DataFrame(label=\"Questions and Agent Answers\", wrap=True)\n",
        "\n",
        "    run_button.click(\n",
        "        fn=run_and_submit_all,\n",
        "        outputs=[status_output, results_table],\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"-\"*30 + \" App Starting \" + \"-\"*30)\n",
        "    # Check for SPACE_HOST and SPACE_ID at startup for information\n",
        "    space_host_startup = os.getenv(\"SPACE_HOST\")\n",
        "    space_id_startup = os.getenv(\"SPACE_ID\") # Get SPACE_ID at startup\n",
        "\n",
        "    if space_host_startup:\n",
        "        print(f\"✅ SPACE_HOST found: {space_host_startup}\")\n",
        "        print(f\"   Runtime URL should be: https://{space_host_startup}.hf.space\")\n",
        "    else:\n",
        "        print(\"ℹ️  SPACE_HOST environment variable not found (running locally?).\")\n",
        "\n",
        "    if space_id_startup: # Print repo URLs if SPACE_ID is found\n",
        "        print(f\"✅ SPACE_ID found: {space_id_startup}\")\n",
        "        print(f\"   Repo URL: https://huggingface.co/spaces/{space_id_startup}\")\n",
        "        print(f\"   Repo Tree URL: https://huggingface.co/spaces/{space_id_startup}/tree/main\")\n",
        "    else:\n",
        "        print(\"ℹ️  SPACE_ID environment variable not found (running locally?). Repo URL cannot be determined.\")\n",
        "\n",
        "    print(\"-\"*(60 + len(\" App Starting \")) + \"\\n\")\n",
        "\n",
        "    print(\"Launching Gradio Interface for Basic Agent Evaluation...\")\n",
        "    demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REywHcMw52Lw",
        "outputId": "f31125e2-2919-4024-ba2b-2e018e91a8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submitting"
      ],
      "metadata": {
        "id": "pm8LeCq84nBz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ggmPZbs5k0Y"
      },
      "outputs": [],
      "source": [
        "!python app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Experiments"
      ],
      "metadata": {
        "id": "UNjfdLMlupAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Playwright"
      ],
      "metadata": {
        "id": "qz0woGa9C8OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!playwright install\n",
        "!playwright install-deps\n",
        "!playwright install chromium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EGm2PzVCK4b",
        "outputId": "e942b8dc-7e6a-470a-e7ee-a8c22834cfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Chromium 136.0.7103.25 (playwright build v1169)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1169/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G167.7 MiB [] 0% 127.6s\u001b[0K\u001b[1G167.7 MiB [] 0% 54.2s\u001b[0K\u001b[1G167.7 MiB [] 0% 62.2s\u001b[0K\u001b[1G167.7 MiB [] 0% 13.5s\u001b[0K\u001b[1G167.7 MiB [] 0% 11.9s\u001b[0K\u001b[1G167.7 MiB [] 0% 9.5s\u001b[0K\u001b[1G167.7 MiB [] 1% 10.4s\u001b[0K\u001b[1G167.7 MiB [] 1% 10.7s\u001b[0K\u001b[1G167.7 MiB [] 1% 8.5s\u001b[0K\u001b[1G167.7 MiB [] 2% 7.2s\u001b[0K\u001b[1G167.7 MiB [] 2% 6.4s\u001b[0K\u001b[1G167.7 MiB [] 3% 5.5s\u001b[0K\u001b[1G167.7 MiB [] 4% 5.0s\u001b[0K\u001b[1G167.7 MiB [] 4% 4.7s\u001b[0K\u001b[1G167.7 MiB [] 5% 4.6s\u001b[0K\u001b[1G167.7 MiB [] 5% 4.4s\u001b[0K\u001b[1G167.7 MiB [] 6% 4.6s\u001b[0K\u001b[1G167.7 MiB [] 6% 4.7s\u001b[0K\u001b[1G167.7 MiB [] 6% 4.6s\u001b[0K\u001b[1G167.7 MiB [] 7% 4.4s\u001b[0K\u001b[1G167.7 MiB [] 8% 4.2s\u001b[0K\u001b[1G167.7 MiB [] 9% 4.3s\u001b[0K\u001b[1G167.7 MiB [] 9% 4.2s\u001b[0K\u001b[1G167.7 MiB [] 10% 4.1s\u001b[0K\u001b[1G167.7 MiB [] 10% 4.0s\u001b[0K\u001b[1G167.7 MiB [] 10% 4.1s\u001b[0K\u001b[1G167.7 MiB [] 11% 4.0s\u001b[0K\u001b[1G167.7 MiB [] 11% 3.9s\u001b[0K\u001b[1G167.7 MiB [] 12% 3.9s\u001b[0K\u001b[1G167.7 MiB [] 13% 3.8s\u001b[0K\u001b[1G167.7 MiB [] 14% 3.6s\u001b[0K\u001b[1G167.7 MiB [] 15% 3.4s\u001b[0K\u001b[1G167.7 MiB [] 16% 3.3s\u001b[0K\u001b[1G167.7 MiB [] 17% 3.2s\u001b[0K\u001b[1G167.7 MiB [] 18% 3.1s\u001b[0K\u001b[1G167.7 MiB [] 19% 3.0s\u001b[0K\u001b[1G167.7 MiB [] 19% 2.9s\u001b[0K\u001b[1G167.7 MiB [] 20% 2.9s\u001b[0K\u001b[1G167.7 MiB [] 21% 2.8s\u001b[0K\u001b[1G167.7 MiB [] 22% 2.8s\u001b[0K\u001b[1G167.7 MiB [] 22% 2.7s\u001b[0K\u001b[1G167.7 MiB [] 23% 2.7s\u001b[0K\u001b[1G167.7 MiB [] 24% 2.7s\u001b[0K\u001b[1G167.7 MiB [] 24% 2.6s\u001b[0K\u001b[1G167.7 MiB [] 25% 2.6s\u001b[0K\u001b[1G167.7 MiB [] 26% 2.6s\u001b[0K\u001b[1G167.7 MiB [] 26% 2.5s\u001b[0K\u001b[1G167.7 MiB [] 27% 2.6s\u001b[0K\u001b[1G167.7 MiB [] 27% 2.5s\u001b[0K\u001b[1G167.7 MiB [] 28% 2.5s\u001b[0K\u001b[1G167.7 MiB [] 29% 2.4s\u001b[0K\u001b[1G167.7 MiB [] 30% 2.3s\u001b[0K\u001b[1G167.7 MiB [] 31% 2.2s\u001b[0K\u001b[1G167.7 MiB [] 32% 2.2s\u001b[0K\u001b[1G167.7 MiB [] 33% 2.1s\u001b[0K\u001b[1G167.7 MiB [] 34% 2.1s\u001b[0K\u001b[1G167.7 MiB [] 34% 2.0s\u001b[0K\u001b[1G167.7 MiB [] 35% 2.0s\u001b[0K\u001b[1G167.7 MiB [] 36% 1.9s\u001b[0K\u001b[1G167.7 MiB [] 37% 1.9s\u001b[0K\u001b[1G167.7 MiB [] 38% 1.8s\u001b[0K\u001b[1G167.7 MiB [] 39% 1.8s\u001b[0K\u001b[1G167.7 MiB [] 40% 1.8s\u001b[0K\u001b[1G167.7 MiB [] 41% 1.7s\u001b[0K\u001b[1G167.7 MiB [] 42% 1.7s\u001b[0K\u001b[1G167.7 MiB [] 43% 1.6s\u001b[0K\u001b[1G167.7 MiB [] 44% 1.6s\u001b[0K\u001b[1G167.7 MiB [] 45% 1.5s\u001b[0K\u001b[1G167.7 MiB [] 46% 1.5s\u001b[0K\u001b[1G167.7 MiB [] 47% 1.5s\u001b[0K\u001b[1G167.7 MiB [] 48% 1.4s\u001b[0K\u001b[1G167.7 MiB [] 49% 1.4s\u001b[0K\u001b[1G167.7 MiB [] 50% 1.4s\u001b[0K\u001b[1G167.7 MiB [] 51% 1.3s\u001b[0K\u001b[1G167.7 MiB [] 53% 1.2s\u001b[0K\u001b[1G167.7 MiB [] 54% 1.2s\u001b[0K\u001b[1G167.7 MiB [] 56% 1.1s\u001b[0K\u001b[1G167.7 MiB [] 57% 1.1s\u001b[0K\u001b[1G167.7 MiB [] 58% 1.1s\u001b[0K\u001b[1G167.7 MiB [] 59% 1.0s\u001b[0K\u001b[1G167.7 MiB [] 60% 1.0s\u001b[0K\u001b[1G167.7 MiB [] 61% 1.0s\u001b[0K\u001b[1G167.7 MiB [] 62% 1.0s\u001b[0K\u001b[1G167.7 MiB [] 63% 1.0s\u001b[0K\u001b[1G167.7 MiB [] 65% 0.9s\u001b[0K\u001b[1G167.7 MiB [] 66% 0.9s\u001b[0K\u001b[1G167.7 MiB [] 67% 0.8s\u001b[0K\u001b[1G167.7 MiB [] 68% 0.8s\u001b[0K\u001b[1G167.7 MiB [] 69% 0.8s\u001b[0K\u001b[1G167.7 MiB [] 70% 0.7s\u001b[0K\u001b[1G167.7 MiB [] 71% 0.7s\u001b[0K\u001b[1G167.7 MiB [] 72% 0.7s\u001b[0K\u001b[1G167.7 MiB [] 73% 0.7s\u001b[0K\u001b[1G167.7 MiB [] 74% 0.6s\u001b[0K\u001b[1G167.7 MiB [] 75% 0.6s\u001b[0K\u001b[1G167.7 MiB [] 76% 0.6s\u001b[0K\u001b[1G167.7 MiB [] 77% 0.6s\u001b[0K\u001b[1G167.7 MiB [] 78% 0.5s\u001b[0K\u001b[1G167.7 MiB [] 79% 0.5s\u001b[0K\u001b[1G167.7 MiB [] 80% 0.5s\u001b[0K\u001b[1G167.7 MiB [] 81% 0.5s\u001b[0K\u001b[1G167.7 MiB [] 82% 0.4s\u001b[0K\u001b[1G167.7 MiB [] 83% 0.4s\u001b[0K\u001b[1G167.7 MiB [] 84% 0.4s\u001b[0K\u001b[1G167.7 MiB [] 85% 0.4s\u001b[0K\u001b[1G167.7 MiB [] 85% 0.3s\u001b[0K\u001b[1G167.7 MiB [] 86% 0.3s\u001b[0K\u001b[1G167.7 MiB [] 87% 0.3s\u001b[0K\u001b[1G167.7 MiB [] 88% 0.3s\u001b[0K\u001b[1G167.7 MiB [] 89% 0.2s\u001b[0K\u001b[1G167.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G167.7 MiB [] 91% 0.2s\u001b[0K\u001b[1G167.7 MiB [] 92% 0.2s\u001b[0K\u001b[1G167.7 MiB [] 93% 0.2s\u001b[0K\u001b[1G167.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G167.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G167.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G167.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G167.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G167.7 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 136.0.7103.25 (playwright build v1169) downloaded to /root/.cache/ms-playwright/chromium-1169\n",
            "Downloading Chromium Headless Shell 136.0.7103.25 (playwright build v1169)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1169/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G101.4 MiB [] 0% 0.0s\u001b[0K\u001b[1G101.4 MiB [] 0% 57.0s\u001b[0K\u001b[1G101.4 MiB [] 0% 24.9s\u001b[0K\u001b[1G101.4 MiB [] 0% 23.1s\u001b[0K\u001b[1G101.4 MiB [] 0% 22.6s\u001b[0K\u001b[1G101.4 MiB [] 0% 21.3s\u001b[0K\u001b[1G101.4 MiB [] 0% 19.1s\u001b[0K\u001b[1G101.4 MiB [] 0% 17.3s\u001b[0K\u001b[1G101.4 MiB [] 1% 14.3s\u001b[0K\u001b[1G101.4 MiB [] 1% 12.2s\u001b[0K\u001b[1G101.4 MiB [] 1% 10.5s\u001b[0K\u001b[1G101.4 MiB [] 2% 8.9s\u001b[0K\u001b[1G101.4 MiB [] 2% 7.3s\u001b[0K\u001b[1G101.4 MiB [] 3% 6.8s\u001b[0K\u001b[1G101.4 MiB [] 4% 6.0s\u001b[0K\u001b[1G101.4 MiB [] 4% 6.1s\u001b[0K\u001b[1G101.4 MiB [] 5% 5.0s\u001b[0K\u001b[1G101.4 MiB [] 5% 4.8s\u001b[0K\u001b[1G101.4 MiB [] 6% 4.6s\u001b[0K\u001b[1G101.4 MiB [] 6% 4.7s\u001b[0K\u001b[1G101.4 MiB [] 7% 4.6s\u001b[0K\u001b[1G101.4 MiB [] 8% 4.3s\u001b[0K\u001b[1G101.4 MiB [] 9% 4.2s\u001b[0K\u001b[1G101.4 MiB [] 9% 4.1s\u001b[0K\u001b[1G101.4 MiB [] 10% 4.1s\u001b[0K\u001b[1G101.4 MiB [] 10% 4.2s\u001b[0K\u001b[1G101.4 MiB [] 11% 4.1s\u001b[0K\u001b[1G101.4 MiB [] 12% 4.0s\u001b[0K\u001b[1G101.4 MiB [] 12% 3.9s\u001b[0K\u001b[1G101.4 MiB [] 13% 3.7s\u001b[0K\u001b[1G101.4 MiB [] 14% 3.5s\u001b[0K\u001b[1G101.4 MiB [] 15% 3.3s\u001b[0K\u001b[1G101.4 MiB [] 16% 3.3s\u001b[0K\u001b[1G101.4 MiB [] 17% 3.3s\u001b[0K\u001b[1G101.4 MiB [] 17% 3.2s\u001b[0K\u001b[1G101.4 MiB [] 18% 3.2s\u001b[0K\u001b[1G101.4 MiB [] 19% 3.0s\u001b[0K\u001b[1G101.4 MiB [] 20% 3.0s\u001b[0K\u001b[1G101.4 MiB [] 22% 2.8s\u001b[0K\u001b[1G101.4 MiB [] 23% 2.7s\u001b[0K\u001b[1G101.4 MiB [] 25% 2.5s\u001b[0K\u001b[1G101.4 MiB [] 26% 2.5s\u001b[0K\u001b[1G101.4 MiB [] 27% 2.5s\u001b[0K\u001b[1G101.4 MiB [] 28% 2.4s\u001b[0K\u001b[1G101.4 MiB [] 29% 2.4s\u001b[0K\u001b[1G101.4 MiB [] 29% 2.3s\u001b[0K\u001b[1G101.4 MiB [] 30% 2.3s\u001b[0K\u001b[1G101.4 MiB [] 31% 2.3s\u001b[0K\u001b[1G101.4 MiB [] 32% 2.2s\u001b[0K\u001b[1G101.4 MiB [] 33% 2.2s\u001b[0K\u001b[1G101.4 MiB [] 33% 2.1s\u001b[0K\u001b[1G101.4 MiB [] 34% 2.1s\u001b[0K\u001b[1G101.4 MiB [] 35% 2.0s\u001b[0K\u001b[1G101.4 MiB [] 36% 2.0s\u001b[0K\u001b[1G101.4 MiB [] 38% 1.9s\u001b[0K\u001b[1G101.4 MiB [] 39% 1.8s\u001b[0K\u001b[1G101.4 MiB [] 40% 1.8s\u001b[0K\u001b[1G101.4 MiB [] 41% 1.7s\u001b[0K\u001b[1G101.4 MiB [] 42% 1.6s\u001b[0K\u001b[1G101.4 MiB [] 43% 1.6s\u001b[0K\u001b[1G101.4 MiB [] 44% 1.6s\u001b[0K\u001b[1G101.4 MiB [] 45% 1.6s\u001b[0K\u001b[1G101.4 MiB [] 46% 1.5s\u001b[0K\u001b[1G101.4 MiB [] 47% 1.5s\u001b[0K\u001b[1G101.4 MiB [] 48% 1.5s\u001b[0K\u001b[1G101.4 MiB [] 49% 1.4s\u001b[0K\u001b[1G101.4 MiB [] 50% 1.4s\u001b[0K\u001b[1G101.4 MiB [] 51% 1.4s\u001b[0K\u001b[1G101.4 MiB [] 52% 1.4s\u001b[0K\u001b[1G101.4 MiB [] 52% 1.3s\u001b[0K\u001b[1G101.4 MiB [] 53% 1.3s\u001b[0K\u001b[1G101.4 MiB [] 54% 1.3s\u001b[0K\u001b[1G101.4 MiB [] 55% 1.3s\u001b[0K\u001b[1G101.4 MiB [] 56% 1.2s\u001b[0K\u001b[1G101.4 MiB [] 57% 1.2s\u001b[0K\u001b[1G101.4 MiB [] 58% 1.2s\u001b[0K\u001b[1G101.4 MiB [] 59% 1.1s\u001b[0K\u001b[1G101.4 MiB [] 60% 1.1s\u001b[0K\u001b[1G101.4 MiB [] 61% 1.1s\u001b[0K\u001b[1G101.4 MiB [] 62% 1.0s\u001b[0K\u001b[1G101.4 MiB [] 63% 1.0s\u001b[0K\u001b[1G101.4 MiB [] 64% 0.9s\u001b[0K\u001b[1G101.4 MiB [] 65% 0.9s\u001b[0K\u001b[1G101.4 MiB [] 66% 0.9s\u001b[0K\u001b[1G101.4 MiB [] 67% 0.8s\u001b[0K\u001b[1G101.4 MiB [] 68% 0.8s\u001b[0K\u001b[1G101.4 MiB [] 70% 0.8s\u001b[0K\u001b[1G101.4 MiB [] 71% 0.7s\u001b[0K\u001b[1G101.4 MiB [] 73% 0.7s\u001b[0K\u001b[1G101.4 MiB [] 74% 0.6s\u001b[0K\u001b[1G101.4 MiB [] 75% 0.6s\u001b[0K\u001b[1G101.4 MiB [] 76% 0.6s\u001b[0K\u001b[1G101.4 MiB [] 77% 0.6s\u001b[0K\u001b[1G101.4 MiB [] 78% 0.5s\u001b[0K\u001b[1G101.4 MiB [] 79% 0.5s\u001b[0K\u001b[1G101.4 MiB [] 81% 0.5s\u001b[0K\u001b[1G101.4 MiB [] 82% 0.4s\u001b[0K\u001b[1G101.4 MiB [] 83% 0.4s\u001b[0K\u001b[1G101.4 MiB [] 85% 0.4s\u001b[0K\u001b[1G101.4 MiB [] 85% 0.3s\u001b[0K\u001b[1G101.4 MiB [] 87% 0.3s\u001b[0K\u001b[1G101.4 MiB [] 88% 0.3s\u001b[0K\u001b[1G101.4 MiB [] 89% 0.3s\u001b[0K\u001b[1G101.4 MiB [] 89% 0.2s\u001b[0K\u001b[1G101.4 MiB [] 91% 0.2s\u001b[0K\u001b[1G101.4 MiB [] 92% 0.2s\u001b[0K\u001b[1G101.4 MiB [] 93% 0.1s\u001b[0K\u001b[1G101.4 MiB [] 94% 0.1s\u001b[0K\u001b[1G101.4 MiB [] 95% 0.1s\u001b[0K\u001b[1G101.4 MiB [] 96% 0.1s\u001b[0K\u001b[1G101.4 MiB [] 97% 0.1s\u001b[0K\u001b[1G101.4 MiB [] 99% 0.0s\u001b[0K\u001b[1G101.4 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 136.0.7103.25 (playwright build v1169) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1169\n",
            "Downloading Firefox 137.0 (playwright build v1482)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1482/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G91 MiB [] 0% 0.0s\u001b[0K\u001b[1G91 MiB [] 0% 21.3s\u001b[0K\u001b[1G91 MiB [] 0% 8.4s\u001b[0K\u001b[1G91 MiB [] 0% 6.2s\u001b[0K\u001b[1G91 MiB [] 1% 6.0s\u001b[0K\u001b[1G91 MiB [] 1% 5.6s\u001b[0K\u001b[1G91 MiB [] 2% 6.4s\u001b[0K\u001b[1G91 MiB [] 2% 5.8s\u001b[0K\u001b[1G91 MiB [] 3% 5.7s\u001b[0K\u001b[1G91 MiB [] 3% 6.0s\u001b[0K\u001b[1G91 MiB [] 3% 6.1s\u001b[0K\u001b[1G91 MiB [] 3% 6.2s\u001b[0K\u001b[1G91 MiB [] 4% 5.7s\u001b[0K\u001b[1G91 MiB [] 5% 4.6s\u001b[0K\u001b[1G91 MiB [] 6% 4.4s\u001b[0K\u001b[1G91 MiB [] 7% 3.7s\u001b[0K\u001b[1G91 MiB [] 9% 3.3s\u001b[0K\u001b[1G91 MiB [] 10% 3.1s\u001b[0K\u001b[1G91 MiB [] 10% 3.2s\u001b[0K\u001b[1G91 MiB [] 10% 3.6s\u001b[0K\u001b[1G91 MiB [] 11% 3.6s\u001b[0K\u001b[1G91 MiB [] 11% 3.5s\u001b[0K\u001b[1G91 MiB [] 12% 3.6s\u001b[0K\u001b[1G91 MiB [] 12% 3.5s\u001b[0K\u001b[1G91 MiB [] 13% 3.4s\u001b[0K\u001b[1G91 MiB [] 14% 3.4s\u001b[0K\u001b[1G91 MiB [] 15% 3.4s\u001b[0K\u001b[1G91 MiB [] 15% 3.3s\u001b[0K\u001b[1G91 MiB [] 16% 3.2s\u001b[0K\u001b[1G91 MiB [] 16% 3.3s\u001b[0K\u001b[1G91 MiB [] 17% 3.3s\u001b[0K\u001b[1G91 MiB [] 18% 3.2s\u001b[0K\u001b[1G91 MiB [] 19% 3.1s\u001b[0K\u001b[1G91 MiB [] 19% 3.0s\u001b[0K\u001b[1G91 MiB [] 20% 2.9s\u001b[0K\u001b[1G91 MiB [] 22% 2.8s\u001b[0K\u001b[1G91 MiB [] 23% 2.7s\u001b[0K\u001b[1G91 MiB [] 24% 2.6s\u001b[0K\u001b[1G91 MiB [] 25% 2.5s\u001b[0K\u001b[1G91 MiB [] 26% 2.4s\u001b[0K\u001b[1G91 MiB [] 27% 2.3s\u001b[0K\u001b[1G91 MiB [] 28% 2.3s\u001b[0K\u001b[1G91 MiB [] 29% 2.3s\u001b[0K\u001b[1G91 MiB [] 30% 2.2s\u001b[0K\u001b[1G91 MiB [] 31% 2.1s\u001b[0K\u001b[1G91 MiB [] 32% 2.1s\u001b[0K\u001b[1G91 MiB [] 33% 2.0s\u001b[0K\u001b[1G91 MiB [] 34% 2.0s\u001b[0K\u001b[1G91 MiB [] 35% 2.0s\u001b[0K\u001b[1G91 MiB [] 35% 1.9s\u001b[0K\u001b[1G91 MiB [] 36% 1.9s\u001b[0K\u001b[1G91 MiB [] 37% 1.9s\u001b[0K\u001b[1G91 MiB [] 38% 1.8s\u001b[0K\u001b[1G91 MiB [] 39% 1.8s\u001b[0K\u001b[1G91 MiB [] 40% 1.8s\u001b[0K\u001b[1G91 MiB [] 42% 1.7s\u001b[0K\u001b[1G91 MiB [] 43% 1.7s\u001b[0K\u001b[1G91 MiB [] 44% 1.7s\u001b[0K\u001b[1G91 MiB [] 45% 1.6s\u001b[0K\u001b[1G91 MiB [] 46% 1.6s\u001b[0K\u001b[1G91 MiB [] 47% 1.6s\u001b[0K\u001b[1G91 MiB [] 48% 1.6s\u001b[0K\u001b[1G91 MiB [] 48% 1.5s\u001b[0K\u001b[1G91 MiB [] 49% 1.5s\u001b[0K\u001b[1G91 MiB [] 50% 1.5s\u001b[0K\u001b[1G91 MiB [] 51% 1.5s\u001b[0K\u001b[1G91 MiB [] 52% 1.5s\u001b[0K\u001b[1G91 MiB [] 53% 1.4s\u001b[0K\u001b[1G91 MiB [] 54% 1.4s\u001b[0K\u001b[1G91 MiB [] 55% 1.4s\u001b[0K\u001b[1G91 MiB [] 56% 1.4s\u001b[0K\u001b[1G91 MiB [] 56% 1.3s\u001b[0K\u001b[1G91 MiB [] 57% 1.3s\u001b[0K\u001b[1G91 MiB [] 58% 1.3s\u001b[0K\u001b[1G91 MiB [] 59% 1.2s\u001b[0K\u001b[1G91 MiB [] 60% 1.2s\u001b[0K\u001b[1G91 MiB [] 61% 1.2s\u001b[0K\u001b[1G91 MiB [] 62% 1.2s\u001b[0K\u001b[1G91 MiB [] 62% 1.1s\u001b[0K\u001b[1G91 MiB [] 63% 1.1s\u001b[0K\u001b[1G91 MiB [] 64% 1.1s\u001b[0K\u001b[1G91 MiB [] 65% 1.0s\u001b[0K\u001b[1G91 MiB [] 66% 1.0s\u001b[0K\u001b[1G91 MiB [] 67% 1.0s\u001b[0K\u001b[1G91 MiB [] 68% 0.9s\u001b[0K\u001b[1G91 MiB [] 69% 0.9s\u001b[0K\u001b[1G91 MiB [] 70% 0.9s\u001b[0K\u001b[1G91 MiB [] 72% 0.8s\u001b[0K\u001b[1G91 MiB [] 73% 0.8s\u001b[0K\u001b[1G91 MiB [] 74% 0.8s\u001b[0K\u001b[1G91 MiB [] 75% 0.7s\u001b[0K\u001b[1G91 MiB [] 76% 0.7s\u001b[0K\u001b[1G91 MiB [] 77% 0.7s\u001b[0K\u001b[1G91 MiB [] 78% 0.6s\u001b[0K\u001b[1G91 MiB [] 79% 0.6s\u001b[0K\u001b[1G91 MiB [] 80% 0.6s\u001b[0K\u001b[1G91 MiB [] 81% 0.5s\u001b[0K\u001b[1G91 MiB [] 82% 0.5s\u001b[0K\u001b[1G91 MiB [] 83% 0.5s\u001b[0K\u001b[1G91 MiB [] 84% 0.5s\u001b[0K\u001b[1G91 MiB [] 84% 0.4s\u001b[0K\u001b[1G91 MiB [] 85% 0.4s\u001b[0K\u001b[1G91 MiB [] 86% 0.4s\u001b[0K\u001b[1G91 MiB [] 87% 0.4s\u001b[0K\u001b[1G91 MiB [] 88% 0.3s\u001b[0K\u001b[1G91 MiB [] 89% 0.3s\u001b[0K\u001b[1G91 MiB [] 90% 0.3s\u001b[0K\u001b[1G91 MiB [] 91% 0.3s\u001b[0K\u001b[1G91 MiB [] 91% 0.2s\u001b[0K\u001b[1G91 MiB [] 92% 0.2s\u001b[0K\u001b[1G91 MiB [] 93% 0.2s\u001b[0K\u001b[1G91 MiB [] 94% 0.2s\u001b[0K\u001b[1G91 MiB [] 95% 0.1s\u001b[0K\u001b[1G91 MiB [] 96% 0.1s\u001b[0K\u001b[1G91 MiB [] 97% 0.1s\u001b[0K\u001b[1G91 MiB [] 98% 0.1s\u001b[0K\u001b[1G91 MiB [] 98% 0.0s\u001b[0K\u001b[1G91 MiB [] 99% 0.0s\u001b[0K\u001b[1G91 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 137.0 (playwright build v1482) downloaded to /root/.cache/ms-playwright/firefox-1482\n",
            "Downloading Webkit 18.4 (playwright build v2158)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2158/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G93.8 MiB [] 0% 0.0s\u001b[0K\u001b[1G93.8 MiB [] 0% 17.3s\u001b[0K\u001b[1G93.8 MiB [] 0% 11.9s\u001b[0K\u001b[1G93.8 MiB [] 0% 8.5s\u001b[0K\u001b[1G93.8 MiB [] 1% 5.8s\u001b[0K\u001b[1G93.8 MiB [] 1% 5.1s\u001b[0K\u001b[1G93.8 MiB [] 2% 4.5s\u001b[0K\u001b[1G93.8 MiB [] 2% 4.8s\u001b[0K\u001b[1G93.8 MiB [] 2% 4.7s\u001b[0K\u001b[1G93.8 MiB [] 3% 4.7s\u001b[0K\u001b[1G93.8 MiB [] 4% 4.1s\u001b[0K\u001b[1G93.8 MiB [] 4% 3.6s\u001b[0K\u001b[1G93.8 MiB [] 6% 3.0s\u001b[0K\u001b[1G93.8 MiB [] 6% 2.9s\u001b[0K\u001b[1G93.8 MiB [] 7% 2.9s\u001b[0K\u001b[1G93.8 MiB [] 8% 2.7s\u001b[0K\u001b[1G93.8 MiB [] 8% 2.8s\u001b[0K\u001b[1G93.8 MiB [] 9% 2.9s\u001b[0K\u001b[1G93.8 MiB [] 9% 3.0s\u001b[0K\u001b[1G93.8 MiB [] 10% 3.4s\u001b[0K\u001b[1G93.8 MiB [] 10% 3.5s\u001b[0K\u001b[1G93.8 MiB [] 10% 3.4s\u001b[0K\u001b[1G93.8 MiB [] 11% 3.3s\u001b[0K\u001b[1G93.8 MiB [] 12% 3.3s\u001b[0K\u001b[1G93.8 MiB [] 13% 3.1s\u001b[0K\u001b[1G93.8 MiB [] 13% 3.2s\u001b[0K\u001b[1G93.8 MiB [] 14% 3.1s\u001b[0K\u001b[1G93.8 MiB [] 15% 3.0s\u001b[0K\u001b[1G93.8 MiB [] 16% 2.9s\u001b[0K\u001b[1G93.8 MiB [] 16% 3.0s\u001b[0K\u001b[1G93.8 MiB [] 17% 2.9s\u001b[0K\u001b[1G93.8 MiB [] 18% 2.8s\u001b[0K\u001b[1G93.8 MiB [] 19% 2.8s\u001b[0K\u001b[1G93.8 MiB [] 20% 2.7s\u001b[0K\u001b[1G93.8 MiB [] 21% 2.6s\u001b[0K\u001b[1G93.8 MiB [] 22% 2.5s\u001b[0K\u001b[1G93.8 MiB [] 23% 2.4s\u001b[0K\u001b[1G93.8 MiB [] 24% 2.4s\u001b[0K\u001b[1G93.8 MiB [] 25% 2.4s\u001b[0K\u001b[1G93.8 MiB [] 25% 2.3s\u001b[0K\u001b[1G93.8 MiB [] 26% 2.3s\u001b[0K\u001b[1G93.8 MiB [] 27% 2.2s\u001b[0K\u001b[1G93.8 MiB [] 28% 2.2s\u001b[0K\u001b[1G93.8 MiB [] 29% 2.1s\u001b[0K\u001b[1G93.8 MiB [] 30% 2.0s\u001b[0K\u001b[1G93.8 MiB [] 31% 2.0s\u001b[0K\u001b[1G93.8 MiB [] 32% 2.0s\u001b[0K\u001b[1G93.8 MiB [] 33% 1.9s\u001b[0K\u001b[1G93.8 MiB [] 35% 1.9s\u001b[0K\u001b[1G93.8 MiB [] 36% 1.8s\u001b[0K\u001b[1G93.8 MiB [] 37% 1.8s\u001b[0K\u001b[1G93.8 MiB [] 38% 1.7s\u001b[0K\u001b[1G93.8 MiB [] 39% 1.7s\u001b[0K\u001b[1G93.8 MiB [] 40% 1.7s\u001b[0K\u001b[1G93.8 MiB [] 41% 1.7s\u001b[0K\u001b[1G93.8 MiB [] 42% 1.6s\u001b[0K\u001b[1G93.8 MiB [] 43% 1.6s\u001b[0K\u001b[1G93.8 MiB [] 44% 1.6s\u001b[0K\u001b[1G93.8 MiB [] 45% 1.5s\u001b[0K\u001b[1G93.8 MiB [] 46% 1.5s\u001b[0K\u001b[1G93.8 MiB [] 47% 1.5s\u001b[0K\u001b[1G93.8 MiB [] 48% 1.4s\u001b[0K\u001b[1G93.8 MiB [] 50% 1.4s\u001b[0K\u001b[1G93.8 MiB [] 51% 1.4s\u001b[0K\u001b[1G93.8 MiB [] 51% 1.3s\u001b[0K\u001b[1G93.8 MiB [] 52% 1.3s\u001b[0K\u001b[1G93.8 MiB [] 53% 1.3s\u001b[0K\u001b[1G93.8 MiB [] 54% 1.3s\u001b[0K\u001b[1G93.8 MiB [] 55% 1.2s\u001b[0K\u001b[1G93.8 MiB [] 56% 1.2s\u001b[0K\u001b[1G93.8 MiB [] 57% 1.2s\u001b[0K\u001b[1G93.8 MiB [] 58% 1.1s\u001b[0K\u001b[1G93.8 MiB [] 60% 1.1s\u001b[0K\u001b[1G93.8 MiB [] 63% 1.0s\u001b[0K\u001b[1G93.8 MiB [] 65% 0.9s\u001b[0K\u001b[1G93.8 MiB [] 68% 0.8s\u001b[0K\u001b[1G93.8 MiB [] 69% 0.7s\u001b[0K\u001b[1G93.8 MiB [] 70% 0.7s\u001b[0K\u001b[1G93.8 MiB [] 71% 0.7s\u001b[0K\u001b[1G93.8 MiB [] 72% 0.7s\u001b[0K\u001b[1G93.8 MiB [] 73% 0.6s\u001b[0K\u001b[1G93.8 MiB [] 74% 0.6s\u001b[0K\u001b[1G93.8 MiB [] 75% 0.6s\u001b[0K\u001b[1G93.8 MiB [] 77% 0.6s\u001b[0K\u001b[1G93.8 MiB [] 77% 0.5s\u001b[0K\u001b[1G93.8 MiB [] 78% 0.5s\u001b[0K\u001b[1G93.8 MiB [] 79% 0.5s\u001b[0K\u001b[1G93.8 MiB [] 80% 0.5s\u001b[0K\u001b[1G93.8 MiB [] 82% 0.4s\u001b[0K\u001b[1G93.8 MiB [] 83% 0.4s\u001b[0K\u001b[1G93.8 MiB [] 84% 0.4s\u001b[0K\u001b[1G93.8 MiB [] 85% 0.3s\u001b[0K\u001b[1G93.8 MiB [] 86% 0.3s\u001b[0K\u001b[1G93.8 MiB [] 87% 0.3s\u001b[0K\u001b[1G93.8 MiB [] 88% 0.3s\u001b[0K\u001b[1G93.8 MiB [] 89% 0.2s\u001b[0K\u001b[1G93.8 MiB [] 90% 0.2s\u001b[0K\u001b[1G93.8 MiB [] 91% 0.2s\u001b[0K\u001b[1G93.8 MiB [] 92% 0.2s\u001b[0K\u001b[1G93.8 MiB [] 93% 0.2s\u001b[0K\u001b[1G93.8 MiB [] 93% 0.1s\u001b[0K\u001b[1G93.8 MiB [] 95% 0.1s\u001b[0K\u001b[1G93.8 MiB [] 96% 0.1s\u001b[0K\u001b[1G93.8 MiB [] 98% 0.0s\u001b[0K\u001b[1G93.8 MiB [] 99% 0.0s\u001b[0K\u001b[1G93.8 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.4 (playwright build v2158) downloaded to /root/.cache/ms-playwright/webkit-2158\n",
            "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 3% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.7s\u001b[0K\u001b[1G2.3 MiB [] 6% 0.9s\u001b[0K\u001b[1G2.3 MiB [] 22% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 38% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 48% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 61% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 67% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 83% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:927:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1047:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:1036:7)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:160:7)\n",
            "Installing dependencies...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,969 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,729 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,549 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,258 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.6 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:23 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.1 kB]\n",
            "Fetched 32.0 MB in 8s (4,070 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-liberation is already the newest version (1:1.07.4-11).\n",
            "libasound2 is already the newest version (1.2.6.1-1ubuntu1).\n",
            "libasound2 set to manually installed.\n",
            "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
            "libatk-bridge2.0-0 set to manually installed.\n",
            "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
            "libatk1.0-0 set to manually installed.\n",
            "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
            "libatspi2.0-0 set to manually installed.\n",
            "libcairo-gobject2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo-gobject2 set to manually installed.\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2 set to manually installed.\n",
            "libegl1 is already the newest version (1.4.0-1).\n",
            "libepoxy0 is already the newest version (1.5.10-1).\n",
            "libepoxy0 set to manually installed.\n",
            "libevent-2.1-7 is already the newest version (2.1.12-stable-1build3).\n",
            "libevent-2.1-7 set to manually installed.\n",
            "libfontconfig1 is already the newest version (2.13.1-4.2ubuntu5).\n",
            "libgles2 is already the newest version (1.4.0-1).\n",
            "libglx0 is already the newest version (1.4.0-1).\n",
            "libglx0 set to manually installed.\n",
            "libicu70 is already the newest version (70.1-2).\n",
            "libicu70 set to manually installed.\n",
            "libjpeg-turbo8 is already the newest version (2.1.2-0ubuntu1).\n",
            "libjpeg-turbo8 set to manually installed.\n",
            "liblcms2-2 is already the newest version (2.12~rc1-2build2).\n",
            "liblcms2-2 set to manually installed.\n",
            "libopengl0 is already the newest version (1.4.0-1).\n",
            "libopus0 is already the newest version (1.3.1-0.1build2).\n",
            "libopus0 set to manually installed.\n",
            "libpng16-16 is already the newest version (1.6.37-3build5).\n",
            "libpng16-16 set to manually installed.\n",
            "libxcb-shm0 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb-shm0 set to manually installed.\n",
            "libxcb1 is already the newest version (1.14-3ubuntu3).\n",
            "libxcb1 set to manually installed.\n",
            "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
            "libxcomposite1 set to manually installed.\n",
            "libxcursor1 is already the newest version (1:1.2.0-2build4).\n",
            "libxcursor1 set to manually installed.\n",
            "libxdamage1 is already the newest version (1:1.1.5-2build2).\n",
            "libxdamage1 set to manually installed.\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxfixes3 is already the newest version (1:6.0.0-1).\n",
            "libxfixes3 set to manually installed.\n",
            "libxi6 is already the newest version (2:1.8-1build1).\n",
            "libxi6 set to manually installed.\n",
            "libxkbcommon0 is already the newest version (1.4.0-1).\n",
            "libxkbcommon0 set to manually installed.\n",
            "libxrandr2 is already the newest version (2:1.5.2-1build1).\n",
            "libxrandr2 set to manually installed.\n",
            "libxrender1 is already the newest version (1:0.9.10-1build4).\n",
            "libx264-163 is already the newest version (2:0.163.3060+git5db6aa6-2build1).\n",
            "libx264-163 set to manually installed.\n",
            "libatomic1 is already the newest version (12.3.0-1ubuntu1~22.04).\n",
            "libatomic1 set to manually installed.\n",
            "libcups2 is already the newest version (2.4.1op1-1ubuntu4.11).\n",
            "libcups2 set to manually installed.\n",
            "libdbus-1-3 is already the newest version (1.12.20-2ubuntu4.1).\n",
            "libdbus-1-3 set to manually installed.\n",
            "libdrm2 is already the newest version (2.4.113-2~ubuntu0.22.04.1).\n",
            "libdrm2 set to manually installed.\n",
            "libfreetype6 is already the newest version (2.11.1+dfsg-1ubuntu0.3).\n",
            "libfreetype6 set to manually installed.\n",
            "libgbm1 is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libgbm1 set to manually installed.\n",
            "libgdk-pixbuf-2.0-0 is already the newest version (2.42.8+dfsg-1ubuntu0.3).\n",
            "libgdk-pixbuf-2.0-0 set to manually installed.\n",
            "libgstreamer-plugins-base1.0-0 is already the newest version (1.20.1-1ubuntu0.4).\n",
            "libgstreamer-plugins-base1.0-0 set to manually installed.\n",
            "libgstreamer1.0-0 is already the newest version (1.20.3-0ubuntu1.1).\n",
            "libgstreamer1.0-0 set to manually installed.\n",
            "libgtk-3-0 is already the newest version (3.24.33-1ubuntu2.2).\n",
            "libgtk-3-0 set to manually installed.\n",
            "libgtk-4-1 is already the newest version (4.6.9+ds-0ubuntu0.22.04.2).\n",
            "libgtk-4-1 set to manually installed.\n",
            "libharfbuzz0b is already the newest version (2.7.4-1ubuntu3.2).\n",
            "libharfbuzz0b set to manually installed.\n",
            "libnspr4 is already the newest version (2:4.35-0ubuntu0.22.04.1).\n",
            "libnspr4 set to manually installed.\n",
            "libnss3 is already the newest version (2:3.98-0ubuntu0.22.04.2).\n",
            "libnss3 set to manually installed.\n",
            "libopenjp2-7 is already the newest version (2.4.0-6ubuntu0.3).\n",
            "libopenjp2-7 set to manually installed.\n",
            "libpango-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpango-1.0-0 set to manually installed.\n",
            "libpangocairo-1.0-0 is already the newest version (1.50.6+ds-2ubuntu1).\n",
            "libpangocairo-1.0-0 set to manually installed.\n",
            "libwayland-client0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-client0 set to manually installed.\n",
            "libwayland-egl1 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-egl1 set to manually installed.\n",
            "libwayland-server0 is already the newest version (1.20.0-1ubuntu0.1).\n",
            "libwayland-server0 set to manually installed.\n",
            "libwebpdemux2 is already the newest version (1.2.2-2ubuntu0.22.04.2).\n",
            "libx11-6 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-6 set to manually installed.\n",
            "libx11-xcb1 is already the newest version (2:1.7.5-1ubuntu0.3).\n",
            "libx11-xcb1 set to manually installed.\n",
            "libxml2 is already the newest version (2.9.13+dfsg-1ubuntu0.7).\n",
            "libxml2 set to manually installed.\n",
            "libxslt1.1 is already the newest version (1.1.34-4ubuntu0.22.04.3).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.14).\n",
            "The following additional packages will be installed:\n",
            "  dictionaries-common glib-networking glib-networking-common\n",
            "  glib-networking-services gsettings-desktop-schemas hunspell-en-us libaa1\n",
            "  libabsl20210324 libaspell15 libcdparanoia0 libdca0 libdv4 libdvdnav4\n",
            "  libdvdread8 libfaad2 libfluidsynth3 libfreeaptx0 libgav1-0 libglib2.0-bin\n",
            "  libglib2.0-dev libglib2.0-dev-bin libgssdp-1.2-0\n",
            "  libgstreamer-plugins-bad1.0-0 libgstreamer-plugins-good1.0-0 libgupnp-1.2-1\n",
            "  libgupnp-igd-1.0-4 libhunspell-1.7-0 libinstpatch-1.0-2 libjson-glib-1.0-0\n",
            "  libjson-glib-1.0-common libkate1 libldacbt-enc2 libltc11 libmjpegutils-2.1-0\n",
            "  libmodplug1 libmpcdec6 libmpeg2encpp-2.1-0 libmplex2-2.1-0 libnice10\n",
            "  libopenh264-6 libopenni2-0 libqrencode4 libsbc1 libsecret-common libshout3\n",
            "  libsoundtouch1 libsoup-3.0-common libsoup2.4-1 libsoup2.4-common libspandsp2\n",
            "  libsrtp2-1 libtag1v5 libtag1v5-vanilla libtext-iconv-perl libv4l-0\n",
            "  libv4lconvert0 libvisual-0.4-0 libvo-aacenc0 libvo-amrwbenc0 libwavpack1\n",
            "  libwebrtc-audio-processing1 libwildmidi2 libyuv0 libzbar0 libzxingcore1\n",
            "  session-migration timgm6mb-soundfont xfonts-encodings xfonts-utils\n",
            "Suggested packages:\n",
            "  ispell | aspell | hunspell wordlist frei0r-plugins gvfs hunspell\n",
            "  openoffice.org-hunspell | openoffice.org-core aspell libdv-bin oss-compat\n",
            "  libdvdcss2 libenchant-2-voikko libglib2.0-doc libgdk-pixbuf2.0-bin\n",
            "  | libgdk-pixbuf2.0-dev libxml2-utils gnome-shell | notification-daemon\n",
            "  libvisual-0.4-plugins libwildmidi-config fluid-soundfont-gm\n",
            "Recommended packages:\n",
            "  fonts-ipafont-mincho fonts-tlwg-loma gstreamer1.0-x aspell-en\n",
            "  | aspell-dictionary | aspell6a-dictionary enchant-2 xdg-user-dirs\n",
            "  gstreamer1.0-gl libmagickcore-6.q16-6-extra\n",
            "The following NEW packages will be installed:\n",
            "  dictionaries-common fonts-freefont-ttf fonts-ipafont-gothic\n",
            "  fonts-noto-color-emoji fonts-tlwg-loma-otf fonts-unifont fonts-wqy-zenhei\n",
            "  glib-networking glib-networking-common glib-networking-services\n",
            "  gsettings-desktop-schemas gstreamer1.0-libav gstreamer1.0-plugins-bad\n",
            "  gstreamer1.0-plugins-base gstreamer1.0-plugins-good hunspell-en-us libaa1\n",
            "  libabsl20210324 libaspell15 libavif13 libcdparanoia0 libdbus-glib-1-2\n",
            "  libdca0 libdv4 libdvdnav4 libdvdread8 libenchant-2-2 libevdev2 libfaad2\n",
            "  libffi7 libfluidsynth3 libfreeaptx0 libgav1-0 libgssdp-1.2-0\n",
            "  libgstreamer-gl1.0-0 libgstreamer-plugins-bad1.0-0\n",
            "  libgstreamer-plugins-good1.0-0 libgudev-1.0-0 libgupnp-1.2-1\n",
            "  libgupnp-igd-1.0-4 libharfbuzz-icu0 libhunspell-1.7-0 libhyphen0\n",
            "  libinstpatch-1.0-2 libjson-glib-1.0-0 libjson-glib-1.0-common libkate1\n",
            "  libldacbt-enc2 libltc11 libmanette-0.2-0 libmjpegutils-2.1-0 libmodplug1\n",
            "  libmpcdec6 libmpeg2encpp-2.1-0 libmplex2-2.1-0 libnice10 libnotify4\n",
            "  libopenh264-6 libopenni2-0 libproxy1v5 libqrencode4 libsbc1 libsecret-1-0\n",
            "  libsecret-common libshout3 libsoundtouch1 libsoup-3.0-0 libsoup-3.0-common\n",
            "  libsoup2.4-1 libsoup2.4-common libspandsp2 libsrtp2-1 libtag1v5\n",
            "  libtag1v5-vanilla libtext-iconv-perl libv4l-0 libv4lconvert0 libvisual-0.4-0\n",
            "  libvo-aacenc0 libvo-amrwbenc0 libwavpack1 libwebrtc-audio-processing1\n",
            "  libwildmidi2 libwoff1 libxtst6 libyuv0 libzbar0 libzxingcore1\n",
            "  session-migration timgm6mb-soundfont xfonts-cyrillic xfonts-encodings\n",
            "  xfonts-scalable xfonts-utils\n",
            "The following packages will be upgraded:\n",
            "  libglib2.0-0 libglib2.0-bin libglib2.0-dev libglib2.0-dev-bin\n",
            "4 upgraded, 94 newly installed, 0 to remove and 96 not upgraded.\n",
            "Need to get 51.6 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-ipafont-gothic all 00303-21ubuntu1 [3,513 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-dev amd64 2.72.4-0ubuntu2.5 [1,743 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-dev-bin amd64 2.72.4-0ubuntu2.5 [116 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-bin amd64 2.72.4-0ubuntu2.5 [81.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglib2.0-0 amd64 2.72.4-0ubuntu2.5 [1,466 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 fonts-noto-color-emoji all 2.047-0ubuntu0.22.04.1 [10.0 MB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-tlwg-loma-otf all 1:0.7.3-1 [107 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-unifont all 1:14.0.01-1 [3,551 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-wqy-zenhei all 0.9.45-8 [7,472 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libproxy1v5 amd64 0.4.17-2 [51.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking-common all 2.72.0-1 [3,718 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking-services amd64 2.72.0-1 [9,982 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 glib-networking amd64 2.72.0-1 [69.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gstreamer1.0-libav amd64 1.20.3-0ubuntu1 [103 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdparanoia0 amd64 3.10.2+debian-14build2 [49.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvisual-0.4-0 amd64 0.4.0-17build2 [108 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gstreamer1.0-plugins-base amd64 1.20.1-1ubuntu0.4 [712 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaa1 amd64 1.4p5-50build1 [51.9 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdv4 amd64 1.0.0-14build1 [61.9 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgstreamer-plugins-good1.0-0 amd64 1.20.3-0ubuntu1.3 [30.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libshout3 amd64 2.4.5-1build3 [54.5 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtag1v5-vanilla amd64 1.11.1+dfsg.1-3ubuntu3 [304 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtag1v5 amd64 1.11.1+dfsg.1-3ubuntu3 [11.5 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libv4lconvert0 amd64 1.22.1-2build1 [82.4 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libv4l-0 amd64 1.22.1-2build1 [44.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsoup2.4-common all 2.74.2-3ubuntu0.4 [4,658 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsoup2.4-1 amd64 2.74.2-3ubuntu0.4 [287 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gstreamer1.0-plugins-good amd64 1.20.3-0ubuntu1.3 [2,010 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libabsl20210324 amd64 0~20210324.2-2ubuntu0.2 [386 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaspell15 amd64 0.60.8-4build1 [325 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgav1-0 amd64 0.17.0-1build1 [336 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libyuv0 amd64 0.0~git20220104.b91df1a-2 [154 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libavif13 amd64 0.9.3-3 [69.5 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdbus-glib-1-2 amd64 0.112-2build1 [65.4 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdvdread8 amd64 6.1.2-1 [55.7 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdvdnav4 amd64 6.1.1-1 [39.3 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 libenchant-2-2 amd64 2.3.2-1ubuntu2 [50.9 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfaad2 amd64 2.10.0-2 [197 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libffi7 amd64 3.3-5ubuntu1 [19.9 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfreeaptx0 amd64 0.1.1-1 [12.9 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgssdp-1.2-0 amd64 1.4.0.1-2build1 [48.9 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgstreamer-gl1.0-0 amd64 1.20.1-1ubuntu0.4 [204 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgupnp-1.2-1 amd64 1.4.3-1 [93.3 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libgupnp-igd-1.0-4 amd64 1.2.0-1build1 [16.8 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libharfbuzz-icu0 amd64 2.7.4-1ubuntu3.2 [5,890 B]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhyphen0 amd64 2.8.8-7build2 [28.2 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-glib-1.0-common all 1.6.6-1build1 [4,432 B]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-glib-1.0-0 amd64 1.6.6-1build1 [69.9 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libkate1 amd64 0.4.1-11build1 [39.4 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libldacbt-enc2 amd64 2.0.2.3+git20200429+ed310a0-4 [24.6 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libltc11 amd64 1.3.1-1 [12.3 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmanette-0.2-0 amd64 0.2.6-3build1 [30.4 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmjpegutils-2.1-0 amd64 1:2.1.0+debian-6build1 [24.1 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmodplug1 amd64 1:0.8.9.0-3 [153 kB]\n",
            "Get:68 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpcdec6 amd64 2:0.1~r495-2 [32.4 kB]\n",
            "Get:69 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmpeg2encpp-2.1-0 amd64 1:2.1.0+debian-6build1 [69.4 kB]\n",
            "Get:70 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmplex2-2.1-0 amd64 1:2.1.0+debian-6build1 [44.4 kB]\n",
            "Get:71 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libnice10 amd64 0.1.18-2 [156 kB]\n",
            "Get:72 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnotify4 amd64 0.7.9-3ubuntu5.22.04.1 [20.3 kB]\n",
            "Get:73 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenh264-6 amd64 2.2.0+dfsg-2 [407 kB]\n",
            "Get:74 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenni2-0 amd64 2.2.0.33+dfsg-15 [389 kB]\n",
            "Get:75 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqrencode4 amd64 4.1.1-1 [24.0 kB]\n",
            "Get:76 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsecret-common all 0.20.5-2 [4,278 B]\n",
            "Get:77 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsecret-1-0 amd64 0.20.5-2 [124 kB]\n",
            "Get:78 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsoundtouch1 amd64 2.3.1+ds1-1 [38.3 kB]\n",
            "Get:79 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsoup-3.0-common all 3.0.7-0ubuntu1 [62.1 kB]\n",
            "Get:80 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsoup-3.0-0 amd64 3.0.7-0ubuntu1 [278 kB]\n",
            "Get:81 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspandsp2 amd64 0.0.6+dfsg-2 [272 kB]\n",
            "Get:82 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsrtp2-1 amd64 2.4.2-2 [40.7 kB]\n",
            "Get:83 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwebrtc-audio-processing1 amd64 0.3.1-0ubuntu5 [291 kB]\n",
            "Get:84 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libwildmidi2 amd64 0.4.3-1 [59.9 kB]\n",
            "Get:85 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:86 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:87 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzbar0 amd64 0.23.92-4build2 [121 kB]\n",
            "Get:88 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzxingcore1 amd64 1.2.0-1 [619 kB]\n",
            "Get:89 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:90 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:91 http://archive.ubuntu.com/ubuntu jammy/universe amd64 xfonts-cyrillic all 1:1.0.5 [386 kB]\n",
            "Get:92 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-scalable all 1:1.0.3-1.2ubuntu1 [306 kB]\n",
            "Get:93 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdca0 amd64 0.0.7-2 [88.2 kB]\n",
            "Get:94 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgstreamer-plugins-bad1.0-0 amd64 1.20.3-0ubuntu1.1 [489 kB]\n",
            "Get:95 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsbc1 amd64 1.5-3build2 [34.4 kB]\n",
            "Get:96 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libvo-aacenc0 amd64 0.1.3-2 [69.4 kB]\n",
            "Get:97 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libvo-amrwbenc0 amd64 0.1.3-2 [68.2 kB]\n",
            "Get:98 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 gstreamer1.0-plugins-bad amd64 1.20.3-0ubuntu1.1 [2,602 kB]\n",
            "Fetched 51.6 MB in 3s (17.2 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-ipafont-gothic.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-ipafont-gothic_00303-21ubuntu1_all.deb ...\n",
            "Unpacking fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "Preparing to unpack .../01-libglib2.0-dev_2.72.4-0ubuntu2.5_amd64.deb ...\n",
            "Unpacking libglib2.0-dev:amd64 (2.72.4-0ubuntu2.5) over (2.72.4-0ubuntu2.4) ...\n",
            "Preparing to unpack .../02-libglib2.0-dev-bin_2.72.4-0ubuntu2.5_amd64.deb ...\n",
            "Unpacking libglib2.0-dev-bin (2.72.4-0ubuntu2.5) over (2.72.4-0ubuntu2.4) ...\n",
            "Preparing to unpack .../03-libglib2.0-bin_2.72.4-0ubuntu2.5_amd64.deb ...\n",
            "Unpacking libglib2.0-bin (2.72.4-0ubuntu2.5) over (2.72.4-0ubuntu2.4) ...\n",
            "Preparing to unpack .../04-libglib2.0-0_2.72.4-0ubuntu2.5_amd64.deb ...\n",
            "Unpacking libglib2.0-0:amd64 (2.72.4-0ubuntu2.5) over (2.72.4-0ubuntu2.4) ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "Preparing to unpack .../05-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../06-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "Preparing to unpack .../07-fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Selecting previously unselected package fonts-noto-color-emoji.\n",
            "Preparing to unpack .../08-fonts-noto-color-emoji_2.047-0ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package fonts-tlwg-loma-otf.\n",
            "Preparing to unpack .../09-fonts-tlwg-loma-otf_1%3a0.7.3-1_all.deb ...\n",
            "Unpacking fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Selecting previously unselected package fonts-unifont.\n",
            "Preparing to unpack .../10-fonts-unifont_1%3a14.0.01-1_all.deb ...\n",
            "Unpacking fonts-unifont (1:14.0.01-1) ...\n",
            "Selecting previously unselected package fonts-wqy-zenhei.\n",
            "Preparing to unpack .../11-fonts-wqy-zenhei_0.9.45-8_all.deb ...\n",
            "Unpacking fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Selecting previously unselected package libproxy1v5:amd64.\n",
            "Preparing to unpack .../12-libproxy1v5_0.4.17-2_amd64.deb ...\n",
            "Unpacking libproxy1v5:amd64 (0.4.17-2) ...\n",
            "Selecting previously unselected package glib-networking-common.\n",
            "Preparing to unpack .../13-glib-networking-common_2.72.0-1_all.deb ...\n",
            "Unpacking glib-networking-common (2.72.0-1) ...\n",
            "Selecting previously unselected package glib-networking-services.\n",
            "Preparing to unpack .../14-glib-networking-services_2.72.0-1_amd64.deb ...\n",
            "Unpacking glib-networking-services (2.72.0-1) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../15-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../16-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package glib-networking:amd64.\n",
            "Preparing to unpack .../17-glib-networking_2.72.0-1_amd64.deb ...\n",
            "Unpacking glib-networking:amd64 (2.72.0-1) ...\n",
            "Selecting previously unselected package gstreamer1.0-libav:amd64.\n",
            "Preparing to unpack .../18-gstreamer1.0-libav_1.20.3-0ubuntu1_amd64.deb ...\n",
            "Unpacking gstreamer1.0-libav:amd64 (1.20.3-0ubuntu1) ...\n",
            "Selecting previously unselected package libcdparanoia0:amd64.\n",
            "Preparing to unpack .../19-libcdparanoia0_3.10.2+debian-14build2_amd64.deb ...\n",
            "Unpacking libcdparanoia0:amd64 (3.10.2+debian-14build2) ...\n",
            "Selecting previously unselected package libvisual-0.4-0:amd64.\n",
            "Preparing to unpack .../20-libvisual-0.4-0_0.4.0-17build2_amd64.deb ...\n",
            "Unpacking libvisual-0.4-0:amd64 (0.4.0-17build2) ...\n",
            "Selecting previously unselected package gstreamer1.0-plugins-base:amd64.\n",
            "Preparing to unpack .../21-gstreamer1.0-plugins-base_1.20.1-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking gstreamer1.0-plugins-base:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Selecting previously unselected package libaa1:amd64.\n",
            "Preparing to unpack .../22-libaa1_1.4p5-50build1_amd64.deb ...\n",
            "Unpacking libaa1:amd64 (1.4p5-50build1) ...\n",
            "Selecting previously unselected package libdv4:amd64.\n",
            "Preparing to unpack .../23-libdv4_1.0.0-14build1_amd64.deb ...\n",
            "Unpacking libdv4:amd64 (1.0.0-14build1) ...\n",
            "Selecting previously unselected package libgstreamer-plugins-good1.0-0:amd64.\n",
            "Preparing to unpack .../24-libgstreamer-plugins-good1.0-0_1.20.3-0ubuntu1.3_amd64.deb ...\n",
            "Unpacking libgstreamer-plugins-good1.0-0:amd64 (1.20.3-0ubuntu1.3) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../25-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libshout3:amd64.\n",
            "Preparing to unpack .../26-libshout3_2.4.5-1build3_amd64.deb ...\n",
            "Unpacking libshout3:amd64 (2.4.5-1build3) ...\n",
            "Selecting previously unselected package libtag1v5-vanilla:amd64.\n",
            "Preparing to unpack .../27-libtag1v5-vanilla_1.11.1+dfsg.1-3ubuntu3_amd64.deb ...\n",
            "Unpacking libtag1v5-vanilla:amd64 (1.11.1+dfsg.1-3ubuntu3) ...\n",
            "Selecting previously unselected package libtag1v5:amd64.\n",
            "Preparing to unpack .../28-libtag1v5_1.11.1+dfsg.1-3ubuntu3_amd64.deb ...\n",
            "Unpacking libtag1v5:amd64 (1.11.1+dfsg.1-3ubuntu3) ...\n",
            "Selecting previously unselected package libv4lconvert0:amd64.\n",
            "Preparing to unpack .../29-libv4lconvert0_1.22.1-2build1_amd64.deb ...\n",
            "Unpacking libv4lconvert0:amd64 (1.22.1-2build1) ...\n",
            "Selecting previously unselected package libv4l-0:amd64.\n",
            "Preparing to unpack .../30-libv4l-0_1.22.1-2build1_amd64.deb ...\n",
            "Unpacking libv4l-0:amd64 (1.22.1-2build1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../31-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsoup2.4-common.\n",
            "Preparing to unpack .../32-libsoup2.4-common_2.74.2-3ubuntu0.4_all.deb ...\n",
            "Unpacking libsoup2.4-common (2.74.2-3ubuntu0.4) ...\n",
            "Selecting previously unselected package libsoup2.4-1:amd64.\n",
            "Preparing to unpack .../33-libsoup2.4-1_2.74.2-3ubuntu0.4_amd64.deb ...\n",
            "Unpacking libsoup2.4-1:amd64 (2.74.2-3ubuntu0.4) ...\n",
            "Selecting previously unselected package gstreamer1.0-plugins-good:amd64.\n",
            "Preparing to unpack .../34-gstreamer1.0-plugins-good_1.20.3-0ubuntu1.3_amd64.deb ...\n",
            "Unpacking gstreamer1.0-plugins-good:amd64 (1.20.3-0ubuntu1.3) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../35-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libabsl20210324:amd64.\n",
            "Preparing to unpack .../36-libabsl20210324_0~20210324.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libabsl20210324:amd64 (0~20210324.2-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../37-libaspell15_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Selecting previously unselected package libgav1-0:amd64.\n",
            "Preparing to unpack .../38-libgav1-0_0.17.0-1build1_amd64.deb ...\n",
            "Unpacking libgav1-0:amd64 (0.17.0-1build1) ...\n",
            "Selecting previously unselected package libyuv0:amd64.\n",
            "Preparing to unpack .../39-libyuv0_0.0~git20220104.b91df1a-2_amd64.deb ...\n",
            "Unpacking libyuv0:amd64 (0.0~git20220104.b91df1a-2) ...\n",
            "Selecting previously unselected package libavif13:amd64.\n",
            "Preparing to unpack .../40-libavif13_0.9.3-3_amd64.deb ...\n",
            "Unpacking libavif13:amd64 (0.9.3-3) ...\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "Preparing to unpack .../41-libdbus-glib-1-2_0.112-2build1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Selecting previously unselected package libdvdread8:amd64.\n",
            "Preparing to unpack .../42-libdvdread8_6.1.2-1_amd64.deb ...\n",
            "Unpacking libdvdread8:amd64 (6.1.2-1) ...\n",
            "Selecting previously unselected package libdvdnav4:amd64.\n",
            "Preparing to unpack .../43-libdvdnav4_6.1.1-1_amd64.deb ...\n",
            "Unpacking libdvdnav4:amd64 (6.1.1-1) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../44-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libenchant-2-2:amd64.\n",
            "Preparing to unpack .../45-libenchant-2-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Selecting previously unselected package libfaad2:amd64.\n",
            "Preparing to unpack .../46-libfaad2_2.10.0-2_amd64.deb ...\n",
            "Unpacking libfaad2:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libffi7:amd64.\n",
            "Preparing to unpack .../47-libffi7_3.3-5ubuntu1_amd64.deb ...\n",
            "Unpacking libffi7:amd64 (3.3-5ubuntu1) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../48-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../49-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../50-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package libfreeaptx0:amd64.\n",
            "Preparing to unpack .../51-libfreeaptx0_0.1.1-1_amd64.deb ...\n",
            "Unpacking libfreeaptx0:amd64 (0.1.1-1) ...\n",
            "Selecting previously unselected package libgssdp-1.2-0:amd64.\n",
            "Preparing to unpack .../52-libgssdp-1.2-0_1.4.0.1-2build1_amd64.deb ...\n",
            "Unpacking libgssdp-1.2-0:amd64 (1.4.0.1-2build1) ...\n",
            "Selecting previously unselected package libgstreamer-gl1.0-0:amd64.\n",
            "Preparing to unpack .../53-libgstreamer-gl1.0-0_1.20.1-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking libgstreamer-gl1.0-0:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Selecting previously unselected package libgupnp-1.2-1:amd64.\n",
            "Preparing to unpack .../54-libgupnp-1.2-1_1.4.3-1_amd64.deb ...\n",
            "Unpacking libgupnp-1.2-1:amd64 (1.4.3-1) ...\n",
            "Selecting previously unselected package libgupnp-igd-1.0-4:amd64.\n",
            "Preparing to unpack .../55-libgupnp-igd-1.0-4_1.2.0-1build1_amd64.deb ...\n",
            "Unpacking libgupnp-igd-1.0-4:amd64 (1.2.0-1build1) ...\n",
            "Selecting previously unselected package libharfbuzz-icu0:amd64.\n",
            "Preparing to unpack .../56-libharfbuzz-icu0_2.7.4-1ubuntu3.2_amd64.deb ...\n",
            "Unpacking libharfbuzz-icu0:amd64 (2.7.4-1ubuntu3.2) ...\n",
            "Selecting previously unselected package libhyphen0:amd64.\n",
            "Preparing to unpack .../57-libhyphen0_2.8.8-7build2_amd64.deb ...\n",
            "Unpacking libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Selecting previously unselected package libjson-glib-1.0-common.\n",
            "Preparing to unpack .../58-libjson-glib-1.0-common_1.6.6-1build1_all.deb ...\n",
            "Unpacking libjson-glib-1.0-common (1.6.6-1build1) ...\n",
            "Selecting previously unselected package libjson-glib-1.0-0:amd64.\n",
            "Preparing to unpack .../59-libjson-glib-1.0-0_1.6.6-1build1_amd64.deb ...\n",
            "Unpacking libjson-glib-1.0-0:amd64 (1.6.6-1build1) ...\n",
            "Selecting previously unselected package libkate1:amd64.\n",
            "Preparing to unpack .../60-libkate1_0.4.1-11build1_amd64.deb ...\n",
            "Unpacking libkate1:amd64 (0.4.1-11build1) ...\n",
            "Selecting previously unselected package libldacbt-enc2:amd64.\n",
            "Preparing to unpack .../61-libldacbt-enc2_2.0.2.3+git20200429+ed310a0-4_amd64.deb ...\n",
            "Unpacking libldacbt-enc2:amd64 (2.0.2.3+git20200429+ed310a0-4) ...\n",
            "Selecting previously unselected package libltc11:amd64.\n",
            "Preparing to unpack .../62-libltc11_1.3.1-1_amd64.deb ...\n",
            "Unpacking libltc11:amd64 (1.3.1-1) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../63-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmanette-0.2-0:amd64.\n",
            "Preparing to unpack .../64-libmanette-0.2-0_0.2.6-3build1_amd64.deb ...\n",
            "Unpacking libmanette-0.2-0:amd64 (0.2.6-3build1) ...\n",
            "Selecting previously unselected package libmjpegutils-2.1-0:amd64.\n",
            "Preparing to unpack .../65-libmjpegutils-2.1-0_1%3a2.1.0+debian-6build1_amd64.deb ...\n",
            "Unpacking libmjpegutils-2.1-0:amd64 (1:2.1.0+debian-6build1) ...\n",
            "Selecting previously unselected package libmodplug1:amd64.\n",
            "Preparing to unpack .../66-libmodplug1_1%3a0.8.9.0-3_amd64.deb ...\n",
            "Unpacking libmodplug1:amd64 (1:0.8.9.0-3) ...\n",
            "Selecting previously unselected package libmpcdec6:amd64.\n",
            "Preparing to unpack .../67-libmpcdec6_2%3a0.1~r495-2_amd64.deb ...\n",
            "Unpacking libmpcdec6:amd64 (2:0.1~r495-2) ...\n",
            "Selecting previously unselected package libmpeg2encpp-2.1-0:amd64.\n",
            "Preparing to unpack .../68-libmpeg2encpp-2.1-0_1%3a2.1.0+debian-6build1_amd64.deb ...\n",
            "Unpacking libmpeg2encpp-2.1-0:amd64 (1:2.1.0+debian-6build1) ...\n",
            "Selecting previously unselected package libmplex2-2.1-0:amd64.\n",
            "Preparing to unpack .../69-libmplex2-2.1-0_1%3a2.1.0+debian-6build1_amd64.deb ...\n",
            "Unpacking libmplex2-2.1-0:amd64 (1:2.1.0+debian-6build1) ...\n",
            "Selecting previously unselected package libnice10:amd64.\n",
            "Preparing to unpack .../70-libnice10_0.1.18-2_amd64.deb ...\n",
            "Unpacking libnice10:amd64 (0.1.18-2) ...\n",
            "Selecting previously unselected package libnotify4:amd64.\n",
            "Preparing to unpack .../71-libnotify4_0.7.9-3ubuntu5.22.04.1_amd64.deb ...\n",
            "Unpacking libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Selecting previously unselected package libopenh264-6:amd64.\n",
            "Preparing to unpack .../72-libopenh264-6_2.2.0+dfsg-2_amd64.deb ...\n",
            "Unpacking libopenh264-6:amd64 (2.2.0+dfsg-2) ...\n",
            "Selecting previously unselected package libopenni2-0:amd64.\n",
            "Preparing to unpack .../73-libopenni2-0_2.2.0.33+dfsg-15_amd64.deb ...\n",
            "Unpacking libopenni2-0:amd64 (2.2.0.33+dfsg-15) ...\n",
            "Selecting previously unselected package libqrencode4:amd64.\n",
            "Preparing to unpack .../74-libqrencode4_4.1.1-1_amd64.deb ...\n",
            "Unpacking libqrencode4:amd64 (4.1.1-1) ...\n",
            "Selecting previously unselected package libsecret-common.\n",
            "Preparing to unpack .../75-libsecret-common_0.20.5-2_all.deb ...\n",
            "Unpacking libsecret-common (0.20.5-2) ...\n",
            "Selecting previously unselected package libsecret-1-0:amd64.\n",
            "Preparing to unpack .../76-libsecret-1-0_0.20.5-2_amd64.deb ...\n",
            "Unpacking libsecret-1-0:amd64 (0.20.5-2) ...\n",
            "Selecting previously unselected package libsoundtouch1:amd64.\n",
            "Preparing to unpack .../77-libsoundtouch1_2.3.1+ds1-1_amd64.deb ...\n",
            "Unpacking libsoundtouch1:amd64 (2.3.1+ds1-1) ...\n",
            "Selecting previously unselected package libsoup-3.0-common.\n",
            "Preparing to unpack .../78-libsoup-3.0-common_3.0.7-0ubuntu1_all.deb ...\n",
            "Unpacking libsoup-3.0-common (3.0.7-0ubuntu1) ...\n",
            "Selecting previously unselected package libsoup-3.0-0:amd64.\n",
            "Preparing to unpack .../79-libsoup-3.0-0_3.0.7-0ubuntu1_amd64.deb ...\n",
            "Unpacking libsoup-3.0-0:amd64 (3.0.7-0ubuntu1) ...\n",
            "Selecting previously unselected package libspandsp2:amd64.\n",
            "Preparing to unpack .../80-libspandsp2_0.0.6+dfsg-2_amd64.deb ...\n",
            "Unpacking libspandsp2:amd64 (0.0.6+dfsg-2) ...\n",
            "Selecting previously unselected package libsrtp2-1:amd64.\n",
            "Preparing to unpack .../81-libsrtp2-1_2.4.2-2_amd64.deb ...\n",
            "Unpacking libsrtp2-1:amd64 (2.4.2-2) ...\n",
            "Selecting previously unselected package libwebrtc-audio-processing1:amd64.\n",
            "Preparing to unpack .../82-libwebrtc-audio-processing1_0.3.1-0ubuntu5_amd64.deb ...\n",
            "Unpacking libwebrtc-audio-processing1:amd64 (0.3.1-0ubuntu5) ...\n",
            "Selecting previously unselected package libwildmidi2:amd64.\n",
            "Preparing to unpack .../83-libwildmidi2_0.4.3-1_amd64.deb ...\n",
            "Unpacking libwildmidi2:amd64 (0.4.3-1) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../84-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../85-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libzbar0:amd64.\n",
            "Preparing to unpack .../86-libzbar0_0.23.92-4build2_amd64.deb ...\n",
            "Unpacking libzbar0:amd64 (0.23.92-4build2) ...\n",
            "Selecting previously unselected package libzxingcore1:amd64.\n",
            "Preparing to unpack .../87-libzxingcore1_1.2.0-1_amd64.deb ...\n",
            "Unpacking libzxingcore1:amd64 (1.2.0-1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../88-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../89-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-cyrillic.\n",
            "Preparing to unpack .../90-xfonts-cyrillic_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-cyrillic (1:1.0.5) ...\n",
            "Selecting previously unselected package xfonts-scalable.\n",
            "Preparing to unpack .../91-xfonts-scalable_1%3a1.0.3-1.2ubuntu1_all.deb ...\n",
            "Unpacking xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Selecting previously unselected package libdca0:amd64.\n",
            "Preparing to unpack .../92-libdca0_0.0.7-2_amd64.deb ...\n",
            "Unpacking libdca0:amd64 (0.0.7-2) ...\n",
            "Selecting previously unselected package libgstreamer-plugins-bad1.0-0:amd64.\n",
            "Preparing to unpack .../93-libgstreamer-plugins-bad1.0-0_1.20.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking libgstreamer-plugins-bad1.0-0:amd64 (1.20.3-0ubuntu1.1) ...\n",
            "Selecting previously unselected package libsbc1:amd64.\n",
            "Preparing to unpack .../94-libsbc1_1.5-3build2_amd64.deb ...\n",
            "Unpacking libsbc1:amd64 (1.5-3build2) ...\n",
            "Selecting previously unselected package libvo-aacenc0:amd64.\n",
            "Preparing to unpack .../95-libvo-aacenc0_0.1.3-2_amd64.deb ...\n",
            "Unpacking libvo-aacenc0:amd64 (0.1.3-2) ...\n",
            "Selecting previously unselected package libvo-amrwbenc0:amd64.\n",
            "Preparing to unpack .../96-libvo-amrwbenc0_0.1.3-2_amd64.deb ...\n",
            "Unpacking libvo-amrwbenc0:amd64 (0.1.3-2) ...\n",
            "Selecting previously unselected package gstreamer1.0-plugins-bad:amd64.\n",
            "Preparing to unpack .../97-gstreamer1.0-plugins-bad_1.20.3-0ubuntu1.1_amd64.deb ...\n",
            "Unpacking gstreamer1.0-plugins-bad:amd64 (1.20.3-0ubuntu1.1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up libfreeaptx0:amd64 (0.1.1-1) ...\n",
            "Setting up libmodplug1:amd64 (1:0.8.9.0-3) ...\n",
            "Setting up libcdparanoia0:amd64 (3.10.2+debian-14build2) ...\n",
            "Setting up libvo-amrwbenc0:amd64 (0.1.3-2) ...\n",
            "Setting up libsbc1:amd64 (1.5-3build2) ...\n",
            "Setting up libproxy1v5:amd64 (0.4.17-2) ...\n",
            "Setting up libtag1v5-vanilla:amd64 (1.11.1+dfsg.1-3ubuntu3) ...\n",
            "Setting up libkate1:amd64 (0.4.1-11build1) ...\n",
            "Setting up libharfbuzz-icu0:amd64 (2.7.4-1ubuntu3.2) ...\n",
            "Setting up libopenni2-0:amd64 (2.2.0.33+dfsg-15) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libqrencode4:amd64 (4.1.1-1) ...\n",
            "Setting up libhyphen0:amd64 (2.8.8-7build2) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "Setting up fonts-noto-color-emoji (2.047-0ubuntu0.22.04.1) ...\n",
            "Setting up libvisual-0.4-0:amd64 (0.4.0-17build2) ...\n",
            "Setting up libglib2.0-0:amd64 (2.72.4-0ubuntu2.5) ...\n",
            "Setting up libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libsrtp2-1:amd64 (2.4.2-2) ...\n",
            "Setting up libffi7:amd64 (3.3-5ubuntu1) ...\n",
            "Setting up libldacbt-enc2:amd64 (2.0.2.3+git20200429+ed310a0-4) ...\n",
            "Setting up fonts-wqy-zenhei (0.9.45-8) ...\n",
            "Setting up libwebrtc-audio-processing1:amd64 (0.3.1-0ubuntu5) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up libglib2.0-bin (2.72.4-0ubuntu2.5) ...\n",
            "Setting up libsoup-3.0-common (3.0.7-0ubuntu1) ...\n",
            "Setting up libmpcdec6:amd64 (2:0.1~r495-2) ...\n",
            "Setting up libspandsp2:amd64 (0.0.6+dfsg-2) ...\n",
            "Setting up libvo-aacenc0:amd64 (0.1.3-2) ...\n",
            "Setting up libgstreamer-plugins-bad1.0-0:amd64 (1.20.3-0ubuntu1.1) ...\n",
            "Setting up libgstreamer-plugins-good1.0-0:amd64 (1.20.3-0ubuntu1.3) ...\n",
            "Setting up libsoundtouch1:amd64 (2.3.1+ds1-1) ...\n",
            "Setting up gstreamer1.0-plugins-base:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Setting up fonts-tlwg-loma-otf (1:0.7.3-1) ...\n",
            "Setting up libdvdread8:amd64 (6.1.2-1) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.112-2build1) ...\n",
            "Setting up libnotify4:amd64 (0.7.9-3ubuntu5.22.04.1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libfaad2:amd64 (2.10.0-2) ...\n",
            "Setting up libshout3:amd64 (2.4.5-1build3) ...\n",
            "Setting up libabsl20210324:amd64 (0~20210324.2-2ubuntu0.2) ...\n",
            "Setting up libltc11:amd64 (1.3.1-1) ...\n",
            "Setting up libsoup2.4-common (2.74.2-3ubuntu0.4) ...\n",
            "Setting up libtag1v5:amd64 (1.11.1+dfsg.1-3ubuntu3) ...\n",
            "Setting up libdv4:amd64 (1.0.0-14build1) ...\n",
            "Setting up fonts-ipafont-gothic (00303-21ubuntu1) ...\n",
            "update-alternatives: using /usr/share/fonts/opentype/ipafont-gothic/ipag.ttf to provide /usr/share/fonts/truetype/fonts-japanese-gothic.ttf (fonts-japanese-gothic.ttf) in auto mode\n",
            "Setting up libwildmidi2:amd64 (0.4.3-1) ...\n",
            "Setting up libopenh264-6:amd64 (2.2.0+dfsg-2) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libzxingcore1:amd64 (1.2.0-1) ...\n",
            "Setting up libv4lconvert0:amd64 (1.22.1-2build1) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libdca0:amd64 (0.0.7-2) ...\n",
            "Setting up libjson-glib-1.0-common (1.6.6-1build1) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up glib-networking-common (2.72.0-1) ...\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up libyuv0:amd64 (0.0~git20220104.b91df1a-2) ...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up gstreamer1.0-libav:amd64 (1.20.3-0ubuntu1) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libmjpegutils-2.1-0:amd64 (1:2.1.0+debian-6build1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libsecret-common (0.20.5-2) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libdvdnav4:amd64 (6.1.1-1) ...\n",
            "Setting up fonts-unifont (1:14.0.01-1) ...\n",
            "Setting up libaa1:amd64 (1.4p5-50build1) ...\n",
            "Setting up libglib2.0-dev-bin (2.72.4-0ubuntu2.5) ...\n",
            "Setting up glib-networking-services (2.72.0-1) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Setting up libmanette-0.2-0:amd64 (0.2.6-3build1) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libjson-glib-1.0-0:amd64 (1.6.6-1build1) ...\n",
            "Setting up libv4l-0:amd64 (1.22.1-2build1) ...\n",
            "Setting up libsecret-1-0:amd64 (0.20.5-2) ...\n",
            "Setting up libgstreamer-gl1.0-0:amd64 (1.20.1-1ubuntu0.4) ...\n",
            "Setting up libglib2.0-dev:amd64 (2.72.4-0ubuntu2.5) ...\n",
            "Setting up libgav1-0:amd64 (0.17.0-1build1) ...\n",
            "Setting up libmpeg2encpp-2.1-0:amd64 (1:2.1.0+debian-6build1) ...\n",
            "Setting up libavif13:amd64 (0.9.3-3) ...\n",
            "Setting up xfonts-cyrillic (1:1.0.5) ...\n",
            "Setting up libmplex2-2.1-0:amd64 (1:2.1.0+debian-6build1) ...\n",
            "Setting up xfonts-scalable (1:1.0.3-1.2ubuntu1) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libzbar0:amd64 (0.23.92-4build2) ...\n",
            "Setting up glib-networking:amd64 (2.72.0-1) ...\n",
            "Setting up libsoup2.4-1:amd64 (2.74.2-3ubuntu0.4) ...\n",
            "Setting up gstreamer1.0-plugins-good:amd64 (1.20.3-0ubuntu1.3) ...\n",
            "Setting up libsoup-3.0-0:amd64 (3.0.7-0ubuntu1) ...\n",
            "Setting up libgssdp-1.2-0:amd64 (1.4.0.1-2build1) ...\n",
            "Setting up libgupnp-1.2-1:amd64 (1.4.3-1) ...\n",
            "Setting up libgupnp-igd-1.0-4:amd64 (1.2.0-1build1) ...\n",
            "Setting up libnice10:amd64 (0.1.18-2) ...\n",
            "Setting up gstreamer1.0-plugins-bad:amd64 (1.20.3-0ubuntu1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "wMWBAj2EDEpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import subprocess\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv(override=True)\n",
        "os.environ['USER_AGENT'] = 'myagent'\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import inspect\n",
        "import tempfile\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.tools import Tool, StructuredTool, tool\n",
        "from typing import Tuple, List\n",
        "from typing_extensions import TypedDict, Annotated, Optional\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
        "from langchain_core.prompt_values import PromptValue\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.graph import START, StateGraph, END\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langchain_community.tools import DuckDuckGoSearchResults, DuckDuckGoSearchRun\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_community.agent_toolkits.playwright.toolkit import PlayWrightBrowserToolkit\n",
        "from langchain_community.tools.playwright.utils import create_async_playwright_browser, create_sync_playwright_browser\n",
        "from langchain_community.document_loaders.wikipedia import WikipediaLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_litellm import ChatLiteLLM\n",
        "from langchain_community.document_loaders.arxiv import ArxivLoader\n",
        "import yt_dlp\n",
        "import time\n",
        "import whisper\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import pprint\n",
        "import json\n",
        "from pyld import jsonld\n",
        "from Bio.PDB import PDBParser\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
        "from IPython.display import Image, display\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import nest_asyncio\n",
        "import math\n",
        "import pdfplumber\n",
        "from pptx import Presentation\n",
        "import zipfile\n",
        "import tempfile\n",
        "import os\n",
        "import xlrd\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
        "from duckduckgo_search.exceptions import DuckDuckGoSearchException\n",
        "from IPython.display import Image, display\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "DELAY = 5\n",
        "TIME_SLEEP = 60/15 + DELAY\n",
        "\n",
        "DEFAULT_API_URL = \"https://agents-course-unit4-scoring.hf.space\"\n",
        "space_host_startup = os.getenv(\"SPACE_HOST\")\n",
        "space_id_startup = os.getenv(\"SPACE_ID\")\n",
        "\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY_1\")\n",
        "\n",
        "model_whisper = whisper.load_model(\"base\")\n",
        "\n",
        "chat_model = ChatLiteLLM(model=\"gemini/gemini-2.0-flash\",\n",
        "                         temperature=0,\n",
        "                         api_key=GEMINI_API_KEY,\n",
        "                         max_retries=10,\n",
        "                         verbose=True)\n",
        "\n",
        "model_image = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "processor_image = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "\n",
        "@tool\n",
        "def add(a: str, b: str) -> str:\n",
        "    \"\"\"Adds two numbers provided as strings and returns the result as a string.\"\"\"\n",
        "    return str(float(a) + float(b))\n",
        "\n",
        "@tool\n",
        "def subtract(a: str, b: str) -> str:\n",
        "    \"\"\"Subtracts the second number from the first, both provided as strings, and returns the result as a string.\"\"\"\n",
        "    return str(float(a) - float(b))\n",
        "\n",
        "@tool\n",
        "def multiply(a: str, b: str) -> str:\n",
        "    \"\"\"Multiplies two numbers provided as strings and returns the result as a string.\"\"\"\n",
        "    return str(float(a) * float(b))\n",
        "\n",
        "@tool\n",
        "def divide(a: str, b: str) -> str:\n",
        "    \"\"\"Divides the first number by the second, both provided as strings, and returns the result as a string. The divisor must not be zero.\"\"\"\n",
        "    return str(float(a) / float(b))\n",
        "\n",
        "@tool\n",
        "def power(a: str, b: str) -> str:\n",
        "    \"\"\"Raises the first number (base) to the power of the second number (exponent), both provided as strings, and returns the result as a string.\"\"\"\n",
        "    return str(float(a) ** float(b))\n",
        "\n",
        "@tool\n",
        "def square_root(a: str) -> str:\n",
        "    \"\"\"Calculates the square root of a number provided as a string and returns the result as a string. The number must be non-negative.\"\"\"\n",
        "    return str(math.sqrt(float(a)))\n",
        "\n",
        "@tool\n",
        "def get_information_from_wikipedia(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Search for relevant Wikipedia pages based on a user query and return their URLs.\n",
        "\n",
        "    This tool uses WikipediaLoader to retrieve up to 10 Wikipedia articles related to the input query.\n",
        "    The output consists of a formatted list of URLs pointing to these articles. These URLs can be used\n",
        "    by web automation tools or agents to navigate, scrape, and extract valuable information such as\n",
        "    text, tables, infoboxes, images, and references that may help answer the user's question.\n",
        "\n",
        "    Args:\n",
        "        query (str): A user-provided search query.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted explanation with a list of Wikipedia URLs related to the query.\n",
        "    \"\"\"\n",
        "    search_docs = WikipediaLoader(query=query, load_max_docs=10).load()\n",
        "    urls = [doc.metadata[\"source\"] for doc in search_docs]\n",
        "    url_list = \"\\n\".join(f\"- {url}\" for url in urls)\n",
        "\n",
        "    return f\"\"\"Wikipedia search completed for query: \"{query}\".\n",
        "\n",
        "You must now browse the following Wikipedia pages to extract detailed information. Use `navigate_browser` to explore each URL:\n",
        "\n",
        "Relevant Wikipedia URLs:\n",
        "{url_list}\n",
        "\n",
        "Instructions:\n",
        "- Visit each URL using `navigate_browser`.\n",
        "- Extract full page content, infoboxes, tables, and references.\n",
        "- Use hyperlinks from the page to recursively follow relevant internal links if necessary.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_arxiv(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Searches the arXiv database for academic papers related to a given query and returns up to 3 relevant results,\n",
        "    including source metadata and a preview of each paper's content.\n",
        "\n",
        "    This function queries the arXiv database for papers matching the provided search term. It retrieves up to 3 papers\n",
        "    with content limited to 10,000 characters each. The results are formatted in XML-like structure with metadata\n",
        "    such as the title, authors, and page number (if available).\n",
        "\n",
        "    Args:\n",
        "        query (str): The search term or topic to query on arXiv (e.g., \"machine learning\").\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string containing the metadata (title, authors, page number) and content preview\n",
        "             for each of the top 3 relevant papers found on arXiv.\n",
        "    \"\"\"\n",
        "    search_docs = ArxivLoader(query=query, load_max_docs=3, doc_content_chars_max=10000).load()\n",
        "    content = \"\"\n",
        "    for doc in search_docs:\n",
        "        content += f\"Title: {doc.metadata['Title']}\\n\"\n",
        "        content += f\"Authors: {doc.metadata['Authors']}\\n\"\n",
        "        content += f\"Summary: {doc.metadata['Summary']}\\n\"\n",
        "        content += f\"Published: {doc.metadata['Published']}\\n\\n\"\n",
        "\n",
        "    return f\"\"\"Arxiv search completed for query: \"{query}\".\n",
        "\n",
        "Relevant Arxiv papers:\n",
        "{content}\n",
        "\n",
        "Instructions:\n",
        "- Search the paper link throught it's title.\n",
        "- Visit each URL using `navigate_browser`.\n",
        "- Extract full page content, infoboxes, tables, and references.\n",
        "- Use hyperlinks from the page to recursively follow relevant internal links if necessary.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_web_page(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Use this to extract text from basic HTML pages. Works well for static websites without JavaScript.\n",
        "\n",
        "    This function retrieves the content of a webpage at the specified URL and extracts all the visible text.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the public webpage to fetch (e.g., \"https://example.com\").\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the visible text content extracted from the webpage.\n",
        "\n",
        "    Example:\n",
        "        url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
        "        page_text = get_web_page(url)\n",
        "        print(page_text)\n",
        "\n",
        "    In this example, the function will return the visible text from the Wikipedia page about \"Python programming language\",\n",
        "    excluding JavaScript-generated or hidden content, formatted as plain text.\n",
        "    \"\"\"\n",
        "    loader = WebBaseLoader(url)\n",
        "    docs = loader.load()\n",
        "    content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "    return f\"\"\"The web page content is:\n",
        "\n",
        "{content}\n",
        "\n",
        "Instructions:\n",
        "If the information is not sufficient:\n",
        "- Visit the URL {url} using `navigate_browser`.\n",
        "- Extract full page content, infoboxes, tables, and references.\n",
        "- Use hyperlinks from the page to recursively follow relevant internal links if necessary.\"\"\"\n",
        "\n",
        "def download_video_from_youtube(url, tempdirname):\n",
        "    \"\"\"\n",
        "    Downloads the best quality video from YouTube (MP4 format) and saves it in the specified directory.\n",
        "\n",
        "    This function fetches the video from the provided YouTube URL, ensuring that the video format is MP4 with a\n",
        "    resolution of at least 1080p. It also downloads subtitles if available, saving them in English as SRT files.\n",
        "\n",
        "    Args:\n",
        "        url (str): The YouTube video URL to download (e.g., \"https://www.youtube.com/watch?v=example\").\n",
        "        tempdirname (str): The directory where the downloaded video and subtitles will be saved.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the downloaded video file (MP4 format).\n",
        "\n",
        "    Example:\n",
        "        url = \"https://www.youtube.com/watch?v=example\"\n",
        "        tempdirname = \"/tmp/videos\"\n",
        "        video_filepath = download_video_from_youtube(url, tempdirname)\n",
        "        print(video_filepath)\n",
        "\n",
        "    In this example, the function will download the best quality video and subtitles from the given YouTube URL\n",
        "    and save them in the specified directory.\n",
        "    \"\"\"\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': '(bestvideo[width>=1080][ext=mp4]/bestvideo)+bestaudio/best', #Ensures best settings\n",
        "        'writesubtitles': True, #Adds a subtitles file if it exists\n",
        "        'writeautomaticsub': True, #Adds auto-generated subtitles file\n",
        "        'subtitle': '--write-sub --sub-lang en', #writes subtitles file in english\n",
        "        'subtitlesformat':'srt', #writes the subtitles file in \"srt\" or \"ass/srt/best\"\n",
        "        'skip_download': False, #skips downloading the video file\n",
        "        \"merge_output_format\": \"mp4\",\n",
        "        'outtmpl':f\"{tempdirname}/%(title)s.%(ext)s\",\n",
        "        'quiet': True,\n",
        "        'cookiefile': \"./cookies.txt\"\n",
        "        }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(url, download=True)\n",
        "        video_title = info_dict.get('title', None)\n",
        "        filepath = ydl.prepare_filename(info_dict)\n",
        "    print(f\"Download {filepath.split('/')[-1]} Successful!\")\n",
        "    return filepath\n",
        "\n",
        "def download_audio_from_youtube(url, tempdirname):\n",
        "    \"\"\"\n",
        "    Downloads the best quality audio (MP3 format) from a YouTube video and saves it in the specified directory.\n",
        "\n",
        "    This function fetches the audio from the provided YouTube URL in the best available format and saves it\n",
        "    as an MP3 file. It does not download the video.\n",
        "\n",
        "    Args:\n",
        "        url (str): The YouTube video URL to extract audio from (e.g., \"https://www.youtube.com/watch?v=example\").\n",
        "        tempdirname (str): The directory where the audio file will be saved.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the downloaded audio file (MP3 format).\n",
        "\n",
        "    Example:\n",
        "        url = \"https://www.youtube.com/watch?v=example\"\n",
        "        tempdirname = \"/tmp/audio\"\n",
        "        audio_filepath = download_audio_from_youtube(url, tempdirname)\n",
        "        print(audio_filepath)\n",
        "\n",
        "    In this example, the function will download the best quality audio (MP3) from the YouTube URL\n",
        "    and save it in the specified directory.\n",
        "    \"\"\"\n",
        "\n",
        "    ydl_opts = {\n",
        "        'extract_audio': True,\n",
        "        'format': 'bestaudio',\n",
        "        'outtmpl': f\"{tempdirname}/%(title)s.mp3\",\n",
        "        'quiet': True,\n",
        "        'cookiefile': \"./cookies.txt\"\n",
        "        }\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info_dict = ydl.extract_info(url, download=True)\n",
        "        audio_title = info_dict.get('title', None)\n",
        "        filepath = ydl.prepare_filename(info_dict)\n",
        "    print(f\"Download {filepath.split('/')[-1]} Successful!\")\n",
        "    return filepath\n",
        "\n",
        "def download_youtube_data(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads the description and comments of a YouTube video using the yt-dlp Python API.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the YouTube video.\n",
        "\n",
        "    Returns:\n",
        "        str: File paths and summary of downloaded content.\n",
        "    \"\"\"\n",
        "    ydl_opts = {\n",
        "        'quiet': True,\n",
        "        'extract_flat': False,\n",
        "        'skip_download': True,\n",
        "        'getcomments': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info = ydl.extract_info(url, download=False)\n",
        "\n",
        "        # Guardar descripción\n",
        "        description = info.get(\"description\", \"No description found.\")\n",
        "\n",
        "        # Guardar comentarios\n",
        "        comments = info.get(\"comments\", [])\n",
        "        comment_texts = [c[\"text\"] for c in comments if \"text\" in c]\n",
        "\n",
        "        return (\n",
        "            f\"Downloaded video data:\\n\\n\"\n",
        "            f\"Description: {description}\\n\"\n",
        "            f\"Comments: {comment_texts}\\n\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting video data: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_youtube(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads a YouTube video, extracts the audio, and returns a transcription of the audio.\n",
        "\n",
        "    This function first downloads the video and audio from the given YouTube URL. Then, it transcribes the\n",
        "    speech from the audio file using a transcription service.\n",
        "\n",
        "    Args:\n",
        "        url (str): The YouTube video URL to download and transcribe (e.g., \"https://www.youtube.com/watch?v=example\").\n",
        "\n",
        "    Returns:\n",
        "        str: The transcription text of the audio content extracted from the YouTube video.\n",
        "\n",
        "    Example:\n",
        "        url = \"https://www.youtube.com/watch?v=example\"\n",
        "        transcription = get_information_from_youtube(url)\n",
        "        print(transcription)\n",
        "\n",
        "    In this example, the function will download the video and audio from YouTube, extract the audio, and\n",
        "    return a transcription of the spoken content in the video.\n",
        "    \"\"\"\n",
        "\n",
        "    tempdirname = tempfile.TemporaryDirectory()\n",
        "    video_filepath = download_video_from_youtube(url, tempdirname)\n",
        "    audio_filepath = download_audio_from_youtube(url, tempdirname)\n",
        "    youtube_data = download_youtube_data(url)\n",
        "\n",
        "    return youtube_data + get_information_from_audio.invoke(audio_filepath)\n",
        "\n",
        "@tool\n",
        "def get_information_from_audio(filepath: str) -> str:\n",
        "    \"\"\"\n",
        "    Transcribes speech from an audio file into text using a speech-to-text model.\n",
        "\n",
        "    This function takes an audio file and converts the spoken content into written text using a speech-to-text model.\n",
        "    It is useful when you need to extract information from voice recordings.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the audio file (e.g., \".wav\", \".mp3\", etc.).\n",
        "                         The file should contain spoken content to be transcribed.\n",
        "\n",
        "    Returns:\n",
        "        str: The transcribed text from the audio file.\n",
        "\n",
        "    Example:\n",
        "        filepath = \"/path/to/audiofile.wav\"\n",
        "        transcription = get_information_from_audio(filepath)\n",
        "        print(transcription)\n",
        "\n",
        "    In this example, the function will transcribe the speech from the specified audio file and return the\n",
        "    transcribed text that can be further analyzed or used in other processes.\n",
        "    \"\"\"\n",
        "    result = model_whisper.transcribe(filepath)\n",
        "    return f\"\"\"Transcription of audio file:\n",
        "\n",
        "{result['text']}\n",
        "\n",
        "Please verify this information by cross-checking with reliable external sources such as web searches, Wikipedia, or other knowledge bases before finalizing your response.\n",
        "If necessary, supplement the transcription with additional relevant information to ensure completeness and accuracy.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_pdf(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts structured text content from a PDF file.\n",
        "\n",
        "    This function reads the content of a PDF document located at the specified file path and extracts the text\n",
        "    from each page. The text is returned as a single string, with the order of pages preserved.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the PDF file to be processed (e.g., \"/path/to/document.pdf\").\n",
        "\n",
        "    Returns:\n",
        "        str: A single string containing the combined text extracted from all pages of the PDF.\n",
        "             The text is ordered by the pages as they appear in the document.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"/path/to/document.pdf\"\n",
        "        extracted_text = get_information_from_pdf(file_path)\n",
        "        print(extracted_text)\n",
        "\n",
        "    In this example, the function will extract all text content from the PDF document,\n",
        "    maintaining the page order, and return it as one continuous string.\n",
        "    \"\"\"\n",
        "\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            content = page.extract_tables()\n",
        "            if content and len(content) > 0:\n",
        "                print(\"Tables were detected in the PDF.\")\n",
        "            else:\n",
        "                print(\"No tables were detected in the PDF..\")\n",
        "                loader = PyPDFLoader(file_path = file_path)\n",
        "                documents = loader.load()\n",
        "                content = '\\n'.join([doc.page_content for doc in documents])\n",
        "\n",
        "    return f\"\"\"The PDF file content:\n",
        "\n",
        "{content}\n",
        "\n",
        "Use this data to answer the question or perform any required analysis.\n",
        "Remember, you can use all available tools to supplement with additional relevant information.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_csv(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Loads a CSV file and returns a structured summary of its tabular content.\n",
        "\n",
        "    This function reads the entire CSV file and returns a textual representation of the data\n",
        "    intended for use by AI agents or downstream systems that operate on text-based tables.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CSV file to load.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string containing the CSV content as a table under the label `data_table`,\n",
        "             along with metadata such as the number of rows and columns.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    return f\"\"\"You are given a table extracted from a CSV file with {df.shape[0]} rows and {df.shape[1]} columns.\n",
        "\n",
        "You are given a table extracted from a CSV file.\n",
        "\n",
        "- Columns: {df.columns.tolist()}\n",
        "- Shape: {df.shape[0]} rows × {df.shape[1]} columns\n",
        "- Missing values: {df.isna().sum().sum()} total\n",
        "\n",
        "You must answer user questions using Python code with pandas-like logic. Perform all necessary computations, filtering, and aggregation using the Python interpreter tool.\n",
        "\n",
        "This dataset may contain missing values. If necessary, handle them appropriately (e.g., by filtering or imputing them) before analysis.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_excel(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts data and, when available, background color metadata from an Excel file (.xlsx or .xls).\n",
        "\n",
        "    This function handles both Excel formats:\n",
        "    - For .xlsx: extracts the full table and cell background colors.\n",
        "    - For .xls: extracts only the tabular data (background colors not supported).\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the Excel file (.xlsx or .xls).\n",
        "\n",
        "    Returns:\n",
        "        str: A detailed string containing:\n",
        "            - Table data extracted from the first sheet.\n",
        "            - If available, a matrix of background cell colors (hex format).\n",
        "            - Summary about number of rows and columns.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[-1].lower()\n",
        "\n",
        "    if ext == \".xlsx\":\n",
        "        # Load with openpyxl to extract both data and colors\n",
        "        workbook = load_workbook(file_path, data_only=True)\n",
        "        sheet = workbook.active\n",
        "        color_data = []\n",
        "\n",
        "        for row in sheet.iter_rows():\n",
        "            row_colors = []\n",
        "            for cell in row:\n",
        "                fill = cell.fill\n",
        "                color = fill.start_color.rgb if fill and fill.start_color and fill.start_color.rgb else \"None\"\n",
        "                row_colors.append(f\"#{color}\" if color != \"None\" else \"None\")\n",
        "            color_data.append(row_colors)\n",
        "\n",
        "        df = pd.read_excel(file_path, engine=\"openpyxl\", header=None)\n",
        "        return f\"\"\"You are given two tables extracted from an Excel file (.xlsx):\n",
        "\n",
        "- `data_table` contains the content of each cell.\n",
        "- `color_table` contains the background color of each cell in ARGB hex format (or \"None\" if not set).\n",
        "\n",
        "\"data_table\": {df.values.tolist()}\n",
        "\"color_table\": {color_data}\n",
        "\n",
        "Both tables are the same size: {df.shape[0]} rows × {df.shape[1]} columns.\n",
        "Use this data to analyze the spreadsheet and answer user questions.\"\"\"\n",
        "\n",
        "    elif ext == \".xls\":\n",
        "        # Load with xlrd (colors not supported)\n",
        "        df = pd.read_excel(file_path, engine=\"xlrd\", header=None)\n",
        "        return f\"\"\"You are given a table extracted from an Excel file (.xls):\n",
        "\n",
        "- Background color metadata is not available for .xls files.\n",
        "\n",
        "\"data_table\": {df.values.tolist()}\n",
        "\n",
        "The table has {df.shape[0]} rows × {df.shape[1]} columns.\n",
        "Use this data to analyze the spreadsheet and answer user questions.\"\"\"\n",
        "\n",
        "    else:\n",
        "        return \"Unsupported file format. Please provide a .xls or .xlsx Excel file.\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_xml(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads the contents of an XML file and returns it as plain text.\n",
        "\n",
        "    This function loads and parses an XML file from the specified file path and returns\n",
        "    its content as a plain text string. The text includes the structure and data contained\n",
        "    within the XML, which can help in understanding the layout and details of the document.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the XML file (e.g., \"/path/to/document.xml\").\n",
        "\n",
        "    Returns:\n",
        "        str: A plain text representation of the XML file's contents, including its structure and data.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        xml_string = f.read()\n",
        "        xml_string = xml_string.replace(\">\", \">\\n\")\n",
        "    return f\"\"\"You are given an XML document converted into JSON format:\n",
        "\n",
        "{xml_string}\n",
        "\n",
        "Use this to answer questions about the document's structure, contents, or metadata.\n",
        "If relevant, identify key entities, attributes, or relationships. \"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_json(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Loads and returns JSON content as a formatted string.\n",
        "\n",
        "    This function opens a JSON file from the specified file path, loads its contents,\n",
        "    and returns the entire JSON structure as a string. The data is formatted as valid JSON,\n",
        "    which is helpful for examining structured data in key-value pairs.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the .json file (e.g., \"/path/to/data.json\").\n",
        "\n",
        "    Returns:\n",
        "        str: A stringified version of the entire JSON data, formatted as a valid JSON string.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"/path/to/data.json\"\n",
        "        json_content = get_information_from_json(file_path)\n",
        "        print(json_content)\n",
        "\n",
        "    In this example, the function will:\n",
        "    - Open the specified JSON file.\n",
        "    - Load and parse its contents.\n",
        "    - Return the data as a JSON-formatted string that can be used for further processing or analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return f\"\"\"The content of the JSON/JSON-LD file has been successfully extracted:\n",
        "\n",
        "{json.dumps(data, indent=2)}\n",
        "\n",
        "Instruction for the agent:\n",
        "\n",
        "1. Scan the JSON content for any fields containing URLs (e.g., 'url', '@id', 'source', 'link').\n",
        "2. For each URL found, you must explore the associated website as thoroughly as possible.\n",
        "   - Do not limit the navigation to the landing page.\n",
        "   - Recursively follow internal links, sections, and dynamically loaded content.\n",
        "   - Extract valuable text, metadata, references, and any structured content.\n",
        "3. Prioritize using the tools in the following order:\n",
        "\n",
        "TOOL PRIORITY ORDER:\n",
        "1. `navigate_browser`– Use for navigating to external websites.\n",
        "2. `get_web_page` – Use for static HTML websites without JavaScript.\n",
        "3. `duckduckgo_web_search_run` – Use when a URL is missing or additional context is needed.\n",
        "\n",
        "If the JSON contains names, terms, or identifiers without direct links, consider searching Wikipedia or arXiv for background knowledge.\n",
        "\n",
        "Important:\n",
        "You must fully explore the websites found in the JSON, including internal resources and linked data. Don't stop at the home or landing page.\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_pdb(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts 3D structural data from a PDB (Protein Data Bank) file.\n",
        "\n",
        "    This function parses a PDB file and generates a human-readable summary of its\n",
        "    molecular structure, including information about chains, residues, and atom positions\n",
        "    in 3D space. The output is particularly useful for understanding the structural\n",
        "    layout of proteins or other molecules stored in the PDB format.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The local path to the .pdb file (e.g., \"/path/to/structure.pdb\").\n",
        "\n",
        "    Returns:\n",
        "        str: A detailed, human-readable summary of the PDB file, listing the chains,\n",
        "             residues, and atoms with their 3D coordinates.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"/path/to/structure.pdb\"\n",
        "        pdb_info = get_information_from_pdb(file_path)\n",
        "        print(pdb_info)\n",
        "\n",
        "    In this example, the function will:\n",
        "    - Parse the PDB file located at the given path.\n",
        "    - Extract information about chains, residues, and atoms, along with their coordinates in 3D space.\n",
        "    - Return a summary of the molecular structure with the positions of atoms in x, y, and z coordinates.\n",
        "    \"\"\"\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"my_protein\", file_path)\n",
        "\n",
        "    info = []\n",
        "\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_info = f\"Chain {chain.id} Residue {residue.resname} {residue.id[1]}:\\n\"\n",
        "                for atom in residue:\n",
        "                    coords = atom.get_coord()\n",
        "                    res_info += f\"  Atom {atom.name}: x={coords[0]:.2f}, y={coords[1]:.2f}, z={coords[2]:.2f}\\n\"\n",
        "                info.append(res_info)\n",
        "\n",
        "    return \"\\n\".join(info)\n",
        "\n",
        "@tool\n",
        "def get_information_from_txt(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts plain text content from a .txt file.\n",
        "\n",
        "    This function loads the full textual content from a local .txt file and returns it\n",
        "    as a single string. It preserves the original formatting and is useful for processing\n",
        "    documents that contain unstructured or natural language text.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Absolute or relative path to the .txt file (e.g., \"documents/file.txt\").\n",
        "\n",
        "    Returns:\n",
        "        str: The complete plain text extracted from the file.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"notes/lecture.txt\"\n",
        "        text = get_information_from_txt(file_path)\n",
        "        print(text)\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to read or analyze the full content of a text file.\n",
        "    - The input is stored in a standard plain-text format (.txt).\n",
        "    \"\"\"\n",
        "    loader = TextLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    content = \"\\n\".join([doc.page_content for doc in documents])\n",
        "    return f\"The TXT file content is:\\n\\n {content}\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_python(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Loads and returns the source code from a Python (.py) file.\n",
        "\n",
        "    This function opens a local Python script from the specified file path, reads its entire\n",
        "    source code, and returns it as a formatted string. It's useful for inspecting, analyzing,\n",
        "    or executing the contents of Python files in later steps.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Absolute or relative path to the Python file (e.g., \"scripts/my_script.py\").\n",
        "\n",
        "    Returns:\n",
        "        str: The full Python source code from the file as a string.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"models/model_utils.py\"\n",
        "        code = get_information_from_python(file_path)\n",
        "        print(code)\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to analyze or execute the contents of a Python script.\n",
        "    - The input is a .py file and contains valid Python code.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        code = f.read()\n",
        "    return f\"\"\"Python source code extracted from: {file_path}\n",
        "\n",
        "{code}\n",
        "\n",
        "Suggested next step:\n",
        "If you want to analyze or run this code, use the python_code_executor tool and pass the code directly as input.\n",
        "\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_docx(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text content from a Microsoft Word (.docx) file.\n",
        "\n",
        "    This function reads a .docx file from the specified file path and returns all\n",
        "    human-readable text as a single string, preserving the logical order of the document.\n",
        "    It is useful when you need to analyze or summarize documents created in Microsoft Word.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Full or relative path to the .docx file (e.g., \"reports/report.docx\").\n",
        "\n",
        "    Returns:\n",
        "        str: The complete extracted text from the Word document.\n",
        "\n",
        "    Example:\n",
        "        file_path = \"contracts/agreement.docx\"\n",
        "        text = get_information_from_docx(file_path)\n",
        "        print(text)\n",
        "\n",
        "    Use this tool when:\n",
        "    - The input file is a .docx Word document.\n",
        "    - You want to extract the entire textual content for processing, querying, or summarization.\n",
        "    \"\"\"\n",
        "    loader = Docx2txtLoader(file_path)\n",
        "    documents = loader.load()\n",
        "    return \"\\n\".join([doc.page_content for doc in documents])\n",
        "\n",
        "@tool\n",
        "def get_information_from_image(file_path: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs visual question answering (VQA) on an image file.\n",
        "\n",
        "    This tool allows the agent to reason about the contents of an image. It takes a path to an image file\n",
        "    and a natural language question, processes the image and text using a vision-language model,\n",
        "    and returns a text-based answer derived from the visual information.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the image file (e.g., \"images/photo.png\").\n",
        "        question (str): Natural language question to ask about the image (e.g., \"What is the person holding?\").\n",
        "\n",
        "    Returns:\n",
        "        str: Answer generated by the vision-language model based on the image and the question.\n",
        "\n",
        "    Example:\n",
        "        answer = get_information_from_image(\"cat.jpg\", \"What color is the cat?\")\n",
        "        print(answer)\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to analyze or describe visual content in an image.\n",
        "    - The user asks a question involving a photo, diagram, chart, screenshot, or other image file.\n",
        "    - Visual reasoning is required to answer the question.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": file_path,\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": question},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    text = processor_image.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    inputs = processor_image(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    generated_ids = model_image.generate(**inputs, max_new_tokens=1024)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    output_text = processor_image.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return f\"VISUAL ANSWER: {output_text}\"\n",
        "\n",
        "@tool\n",
        "def get_information_from_pptx(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts all text from a PowerPoint (.pptx) file slide by slide.\n",
        "\n",
        "    Use this tool when:\n",
        "    - You need to read and analyze the text content of a presentation.\n",
        "    - You want to understand slide structure or extract meaningful information from slides.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the .pptx file.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text content from the presentation.\n",
        "    \"\"\"\n",
        "    prs = Presentation(file_path)\n",
        "    all_text = []\n",
        "\n",
        "    for i, slide in enumerate(prs.slides):\n",
        "        slide_text = []\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                slide_text.append(shape.text.strip())\n",
        "        if slide_text:\n",
        "            all_text.append(f\"- Slide {i + 1}\\n\" + \"\\n\".join(slide_text))\n",
        "    all_text = '\\n\\n'.join(all_text)\n",
        "    return f\"\"\"The PPTX file content is:\n",
        "\n",
        "{all_text}\n",
        "\n",
        "You can use this information to answer the user question.\n",
        "\"\"\"\n",
        "\n",
        "@tool\n",
        "def get_all_files_from_zip(file_path: str) -> Tuple[List[str], str]:\n",
        "    \"\"\"\n",
        "    Extracts all files from a ZIP archive to a temporary directory and returns their paths.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the .zip archive.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], str]:\n",
        "            - A list of full paths to the extracted files.\n",
        "            - A summary string indicating how many files were extracted.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    temp_dir = tempfile.gettempdir()\n",
        "    zip_ref = zipfile.ZipFile(file_path, 'r')\n",
        "    zip_ref.extractall(temp_dir)\n",
        "    zip_files = zip_ref.namelist()\n",
        "    for filename in zip_ref.namelist():\n",
        "        temp_file_path = os.path.join(temp_dir, filename)\n",
        "        files.append(temp_file_path)\n",
        "\n",
        "    prompt = f\"The ZIP file contains {len(files)} file(s).\"\n",
        "    return files, prompt\n",
        "\n",
        "\"\"\"# OCR from images\n",
        "OCR_tool = Tool(\n",
        "    name=\"get_text_from_image\",\n",
        "    func=ocr_image,\n",
        "    description=(\n",
        "        \"Uses OCR to extract text from an image file. Input should be the path to a local image (e.g., JPG, PNG).\"\n",
        "    )\n",
        ")\"\"\"\n",
        "\n",
        "duckduckgosearchrun_tool = DuckDuckGoSearchRun()\n",
        "duckduckgosearchresults_tool = DuckDuckGoSearchResults(max_results=5, output_format='list')\n",
        "\n",
        "@retry(\n",
        "    wait=wait_exponential(multiplier=1, min=15, max=60),\n",
        "    stop=stop_after_attempt(3),\n",
        "    retry=retry_if_exception_type(DuckDuckGoSearchException)\n",
        ")\n",
        "@sleep_and_retry\n",
        "@limits(calls=1, period=30)\n",
        "def get_dgg_search(query: str, mode: str):\n",
        "    if mode == \"run\":\n",
        "        return duckduckgosearchrun_tool.run(query)\n",
        "    elif mode == \"results\":\n",
        "        return duckduckgosearchresults_tool.run(query)\n",
        "\n",
        "def get_information_from_searches(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform a DuckDuckGo search and return both a textual summary and structured results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "        str: Combined summary and list of links with navigation instructions.\n",
        "    \"\"\"\n",
        "    text_summary = get_dgg_search(query, mode=\"run\")\n",
        "    structured_results = get_dgg_search(query, mode=\"results\")\n",
        "\n",
        "    formatted_links = \"\\n\".join([\n",
        "        f\"- [{res['title']}]({res['link']}):\\n{res['snippet']}\"\n",
        "        for res in structured_results])\n",
        "\n",
        "    text_summary = \"No results found.\" if len(text_summary) < 5 else text_summary\n",
        "    formatted_links = \"No results found.\" if len(formatted_links) < 5 else formatted_links\n",
        "\n",
        "    return f\"\"\"DuckDuckGo Search Results for: \"{query}\"\n",
        "\n",
        "Summary:\n",
        "{text_summary}\n",
        "\n",
        "Top Links:\n",
        "{formatted_links}\n",
        "\n",
        "Instructions for the Agent:\n",
        "If the summary is insufficient or lacks detail:\n",
        "- Use `navigate_browser` to open one or more of the above links.\n",
        "- Extract the full page content, including main text, tables, and sidebars.\n",
        "- Optionally, explore internal links within those pages if relevant to the query.\n",
        "\"\"\"\n",
        "\n",
        "search_tool = Tool(\n",
        "    name=\"duckduckgo_search\",\n",
        "    func=get_information_from_searches,\n",
        "    description=(\n",
        "        \"Use this tool to perform a live DuckDuckGo web search and retrieve both a summary and a list of relevant links \"\n",
        "        \"with snippets. Ideal for finding general information from public websites that are well indexed by DuckDuckGo.\\n\\n\"\n",
        "        \"Limitations:\\n\"\n",
        "        \"- DuckDuckGo may not return results for sites like Google.\\n\"\n",
        "        \"Fallback Instructions:\\n\"\n",
        "        \"If this tool returns no useful information or fails to retrieve results:\\n\"\n",
        "        \"- Use `navigate_browser` to open relevant links or perform a direct search in a browser context.\\n\"\n",
        "        \"- Extract detailed information from the visited web pages, including main text, sidebars, and tables.\\n\"\n",
        "        \"- Explore internal links within the pages if needed to answer the question.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Python interpreter\n",
        "def verbose_python_executor(code: str) -> str:\n",
        "    \"\"\"Executes Python code and returns both the code and the output.\"\"\"\n",
        "    tool = PythonREPLTool()\n",
        "    result = tool.run(code)\n",
        "    return f\"Code executed:\\n```python\\n{code}\\n```\\n\\nOutput:\\n{result}\"\n",
        "\n",
        "python_tool = Tool(\n",
        "    name=\"python_code_executor\",\n",
        "    func=verbose_python_executor,\n",
        "    description=(\n",
        "        \"Use this tool to execute raw Python code.\\n\\n\"\n",
        "        \"Input: A Python code snippet as a single string.\\n\\n\"\n",
        "        \"Important:\\n\"\n",
        "        \"- This is NOT a remote API call. Do NOT include calls to `default_api`, `__arg1`, or any tool inside the code.\\n\"\n",
        "        \"- The code should be ready to run directly in Python, using standard libraries.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "def download_file_temp(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Downloads a file from the given URL and saves it into a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the file to download.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded file or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Create a temporary directory\n",
        "        temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Extract filename from URL or fallback to a generic name\n",
        "        filename = url.split(\"/\")[-1] or \"downloaded_file\"\n",
        "\n",
        "        # Full path for saving file\n",
        "        file_path = os.path.join(temp_dir, filename)\n",
        "\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        return f\"File downloaded successfully and saved at {file_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Failed to download file: {e}\"\n",
        "\n",
        "download_tool = Tool(\n",
        "    name=\"file_downloader_temp\",\n",
        "    func=download_file_temp,\n",
        "    description=\"Downloads a file from a URL and saves it in a temporary folder.\"\n",
        ")\n",
        "\n",
        "# Create a Playwright browser instance\n",
        "def initialize_web_tools():\n",
        "    async_browser = create_async_playwright_browser()\n",
        "    toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
        "    return toolkit.get_tools()\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list[AnyMessage], add_messages]\n",
        "    question: Optional[str]\n",
        "    file_path: Optional[str]\n",
        "    task_id: Optional[str]\n",
        "    new_messages: Optional[int]\n",
        "    final_answer: Optional[str]\n",
        "\n",
        "class MyAgent:\n",
        "    def __init__(self, web_tools=None):\n",
        "        print(\"MyAgent initialized.\")\n",
        "\n",
        "        self.chat = chat_model\n",
        "\n",
        "        self.tools = [search_tool,\n",
        "                    download_tool,\n",
        "                    get_web_page,\n",
        "                    add,\n",
        "                    subtract,\n",
        "                    multiply,\n",
        "                    divide,\n",
        "                    power,\n",
        "                    square_root,\n",
        "                    get_information_from_wikipedia,\n",
        "                    get_information_from_arxiv,\n",
        "                    get_information_from_youtube,\n",
        "                    python_tool,\n",
        "                    get_information_from_json,\n",
        "                    get_information_from_audio,\n",
        "                    get_information_from_xml,\n",
        "                    get_information_from_docx,\n",
        "                    get_information_from_txt,\n",
        "                    get_information_from_pdf,\n",
        "                    get_information_from_csv,\n",
        "                    get_information_from_excel,\n",
        "                    get_information_from_pdb,\n",
        "                    get_information_from_image,\n",
        "                    get_information_from_pptx,\n",
        "                    get_all_files_from_zip\n",
        "                    ] + web_tools\n",
        "\n",
        "        self.chat_with_tools = self.chat.bind_tools(self.tools, verbose=True)\n",
        "\n",
        "        self.builder = StateGraph(AgentState)\n",
        "        self.builder.add_node(\"assistant\", self.assistant)\n",
        "        self.builder.add_node(\"tools\", ToolNode(self.tools))\n",
        "        self.builder.add_node(\"extract_data_from_file\", self.extract_data_from_file)\n",
        "        self.builder.add_node(\"postprocess\", self.postprocess)\n",
        "\n",
        "        self.builder.add_edge(START, \"extract_data_from_file\")\n",
        "        self.builder.add_edge(\"extract_data_from_file\", \"assistant\")\n",
        "        self.builder.add_conditional_edges(\n",
        "            \"assistant\",\n",
        "            self.assistant_router,\n",
        "            {\n",
        "                \"tools\": \"tools\",\n",
        "                \"postprocess\": \"postprocess\"\n",
        "            }\n",
        "        )\n",
        "        self.builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "        self.agent = self.builder.compile()\n",
        "\n",
        "    async def __call__(self, question: str, file_path: str, task_id: str) -> str:\n",
        "        print(\"\\033[1m\\033[93m\"+\"=\"*150+\"\\033[0m\")\n",
        "        print(f\"QUESTION: {question}\")\n",
        "        print(f\"File: {file_path}\")\n",
        "        prompt = f\"\"\"You are a general AI assistant. I will ask you a question and I give you the extracted data from associate files to the user question.\n",
        "\n",
        "You must follow this process:\n",
        "1. Analyze the user question to identify the required output type (e.g., number, string, list) and key concepts.\n",
        "2. BEFORE planning or using any tool, determine if the answer can be obtained directly from your own knowledge or reasoning. You can't guest the answer.\n",
        "   - If yes, answer directly without using any tools.\n",
        "   - If not, proceed with tool-based planning as described below.\n",
        "3. If tools are needed, generate a plan that includes:\n",
        "   - The approach to solve the question.\n",
        "   - Which tools to use and in what order.\n",
        "   - How to reformulate the query if needed for web search.\n",
        "   - Ensure that search queries do not contain punctuation marks, commas, quotes, or special characters.\n",
        "4. Do not execute any tool until the plan is complete.\n",
        "5. Follow the plan exactly. If a tool fails (e.g., due to network or no results), pause, replan, and retry with:\n",
        "   - A reformulated query (more specific or with synonyms/context).\n",
        "   - A fallback tool if available.\n",
        "6. Evaluate tool results for relevance. If multiple source links are available, explore each one using `navigate_browser` to gather all relevant information.\n",
        "\n",
        "Report your thoughts, and finish your answer with the following template: FINAL ANSWER: [YOUR FINAL ANSWER].\n",
        "\n",
        "YOUR FINAL ANSWER should be a number OR as few words as possible OR a comma separated list of numbers and/or strings.\n",
        "If you are asked for a number, don't use comma to write your number neither use units such as $ or percent sign unless specified otherwise.\n",
        "If you are asked for a string, don't use articles, neither abbreviations (e.g. for cities), and write the digits in plain text unless specified otherwise.\n",
        "If you are asked for a comma separated list, apply the above rules depending of whether the element to be put in the list is a number or a string.\"\"\"\n",
        "\n",
        "        user_message = f\"Question: {question}\\nFilepath: {file_path}\"\n",
        "\n",
        "        messages = [SystemMessage(content=prompt, name=\"SYSTEM\"),\n",
        "                    HumanMessage(content=user_message, name=\"USER\")]\n",
        "\n",
        "        response = await self.agent.ainvoke({\"messages\": messages,\n",
        "                                             \"question\": question,\n",
        "                                             \"file_path\": file_path,\n",
        "                                             \"task_id\": task_id,\n",
        "                                             \"new_messages\": 1},\n",
        "                                              {\"recursion_limit\": 100})\n",
        "\n",
        "        print(\"\\033[1m\\033[93m\"+\"=\"*150+\"\\033[0m\")\n",
        "        return response['messages'][-1].content\n",
        "\n",
        "    async def assistant(self, state: AgentState):\n",
        "        new_messages = state[\"new_messages\"]\n",
        "\n",
        "        for i in reversed(range(1, new_messages+1)):\n",
        "            print(\"\\033[1m\\033[92m\"+\"+\"*150+\"\\033[0m\")\n",
        "            name = state[\"messages\"][-i].name\n",
        "            content = state[\"messages\"][-i].content\n",
        "            print(f'\\033[1m\\033[96m{name}\\033[0m: {content if len(content) < 5000 else content[:5000]}')\n",
        "\n",
        "        result = await self.chat_with_tools.ainvoke(state[\"messages\"])\n",
        "        result.name=\"ASSISTANT\"\n",
        "        time.sleep(TIME_SLEEP)\n",
        "\n",
        "        print(\"\\033[1m\\033[92m\"+\"+\"*150+\"\\033[0m\")\n",
        "        content = result.content[:-2] if result.content[-2:] == '\\n\\n' else result.content\n",
        "        print(f'\\033[1m\\033[96m{result.name}\\033[0m: {content}')\n",
        "        new_messages = 1\n",
        "\n",
        "        return {\"messages\": result, \"new_messages\": new_messages}\n",
        "\n",
        "    def extract_data_from_file(self, state: AgentState) -> str:\n",
        "        path = state[\"file_path\"]\n",
        "        new_messages = state[\"new_messages\"]\n",
        "        prompt = \"\"\n",
        "        messages = []\n",
        "\n",
        "        if path and \".\" in path:\n",
        "            ext = path.strip().split(\".\")[-1].lower()\n",
        "            print(f\"Extension detected: {ext}\")\n",
        "\n",
        "            if ext == \"zip\":\n",
        "                files, prompt = get_all_files_from_zip(path)\n",
        "                name = \"get_all_file_from_zip\"\n",
        "                messages.append(AIMessage(content=prompt, name=name))\n",
        "            else:\n",
        "                files = [path]\n",
        "\n",
        "            for file_path in files:\n",
        "                ext = file_path.strip().split(\".\")[-1].lower()\n",
        "                print(f\"Extension detected: {ext}\")\n",
        "\n",
        "                prompt = f\"Information extracted from {file_path}.\\n\\n\"\n",
        "                match ext:\n",
        "                    case \"csv\":\n",
        "                        content = get_information_from_csv.invoke(file_path)\n",
        "                        name = \"get_information_from_csv\"\n",
        "                    case \"txt\":\n",
        "                        content = get_information_from_txt.invoke(file_path)\n",
        "                        name = \"get_information_from_txt\"\n",
        "                    case \"pdf\":\n",
        "                        content = get_information_from_pdf.invoke(file_path)\n",
        "                        name = \"get_information_from_pdf\"\n",
        "                    case \"json\":\n",
        "                        content = get_information_from_json.invoke(file_path)\n",
        "                        name = \"get_information_from_json\"\n",
        "                    case \"jsonld\":\n",
        "                        content = get_information_from_json.invoke(file_path)\n",
        "                        name = \"get_information_from_json\"\n",
        "                    case \"xml\":\n",
        "                        content = get_information_from_xml.invoke(file_path)\n",
        "                        name = \"get_information_from_xml\"\n",
        "                    case \"pdb\":\n",
        "                        content = get_information_from_pdb.invoke(file_path)\n",
        "                        name = \"get_information_from_pdb\"\n",
        "                    case \"mp3\":\n",
        "                        content = get_information_from_audio.invoke(file_path)\n",
        "                        name = \"get_information_from_audio\"\n",
        "                    case \"m4a\":\n",
        "                        content = get_information_from_audio.invoke(file_path)\n",
        "                        name = \"get_information_from_audio\"\n",
        "                    case \"docx\":\n",
        "                        content = get_information_from_docx.invoke(file_path)\n",
        "                        name = \"get_information_from_docx\"\n",
        "                    case \"xlsx\":\n",
        "                        content = get_information_from_excel.invoke(file_path)\n",
        "                        name = \"get_information_from_excel\"\n",
        "                    case \"xls\":\n",
        "                        content = get_information_from_excel.invoke(file_path)\n",
        "                        name = \"get_information_from_excel\"\n",
        "                    case \"png\":\n",
        "                        content = get_information_from_image.invoke({\"file_path\": file_path, \"question\": state[\"question\"]})\n",
        "                        name = \"get_information_from_image\"\n",
        "                    case \"jpg\":\n",
        "                        content = get_information_from_image.invoke({\"file_path\": file_path, \"question\": state[\"question\"]})\n",
        "                        name = \"get_information_from_image\"\n",
        "                    case \"py\":\n",
        "                        content = get_information_from_python.invoke(file_path)\n",
        "                        name = \"get_information_from_python\"\n",
        "                    case \"pptx\":\n",
        "                        content = get_information_from_pptx.invoke(file_path)\n",
        "                        name = \"get_information_from_pptx\"\n",
        "                    case _:\n",
        "                        content = \"Try to use some available tool to answer the user question.\"\n",
        "                        name = \"handle_no_file\"\n",
        "                prompt += f\"{content}\"\n",
        "                messages.append(AIMessage(content=prompt, name=name))\n",
        "                new_messages += 1\n",
        "        else:\n",
        "            prompt = \"The question doesn't have an attached file.\"\n",
        "            name = \"handle_no_file\"\n",
        "\n",
        "        return {\"messages\": messages, \"new_messages\": new_messages}\n",
        "\n",
        "    def assistant_router(self, state: AgentState) -> str:\n",
        "        tool_decision = tools_condition(state)\n",
        "        if tool_decision == \"tools\":\n",
        "            return \"tools\"\n",
        "        else:\n",
        "            return \"postprocess\"\n",
        "\n",
        "    def postprocess(self, state: AgentState) -> AgentState:\n",
        "        last_msg = state[\"messages\"][-1]\n",
        "        content = last_msg.content\n",
        "        index = content.find(\"FINAL ANSWER: \")\n",
        "        if index != -1:\n",
        "            content = content[index+len(\"FINAL ANSWER: \"):].replace(\"\\n\", \"\")\n",
        "        else:\n",
        "            content = \"Unable to find the answer\"\n",
        "\n",
        "        state[\"messages\"][-1].content = content\n",
        "        return state\n",
        "\n",
        "    def draw_graph(self):\n",
        "        display(Image(self.agent.get_graph().draw_mermaid_png()))\n",
        "        return\n",
        "\n",
        "web_tools= initialize_web_tools()\n",
        "agent = MyAgent(web_tools=web_tools)\n",
        "result = await agent(\"\"\"Hi, I'm making a pie but I could use some help with my shopping list. I have everything I need for the crust, but I'm not sure about the filling. I got the recipe from my friend Aditi, but she left it as a voice memo and the speaker on my phone is buzzing so I can't quite make out what she's saying. Could you please listen to the recipe and list all of the ingredients that my friend described? I only want the ingredients for the filling, as I have everything I need to make my favorite pie crust. I've attached the recipe as Strawberry pie.mp3.\n",
        "\n",
        "In your response, please only list the ingredients, not any measurements. So if the recipe calls for \"a pinch of salt\" or \"two cups of ripe strawberries\" the ingredients on the list would be \"salt\" and \"ripe strawberries\".\n",
        "\n",
        "Please format your response as a comma separated list of ingredients. Also, please alphabetize the ingredients.\"\"\",\n",
        "                    gaia_files + \"99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\",\n",
        "                    None)\n",
        "pprint.pp(result)\n"
      ],
      "metadata": {
        "id": "h6kqgJbPbcKO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9a81031f77d646839cf678e683e83b50",
            "6cf9050c5b114889a13a94c80d993f1b",
            "960c75fe623847c5b874a905da553c07",
            "4888175ef94f46c4ba757a97ce0ea16d",
            "fad406a35fea4cafaad4b33457a7e301",
            "d4257164aeab44fdaf697c9e4d9e2058",
            "2470a40a507747d1aeafb1171dbba19f",
            "fc20cbbc2e664297b12e247eb5a4c815",
            "f035e73dc7384ad9b776e5dbb0eed64e",
            "d7dcdee0b1fe49ef85eeb49be8d14f8c",
            "ef5e948e163245eb8bdd2c5433d76f40"
          ]
        },
        "outputId": "d84f1a02-2beb-4ae4-9195-d07d7b6ae3f9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a81031f77d646839cf678e683e83b50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyAgent initialized.\n",
            "\u001b[1m\u001b[93m======================================================================================================================================================\u001b[0m\n",
            "QUESTION: Hi, I'm making a pie but I could use some help with my shopping list. I have everything I need for the crust, but I'm not sure about the filling. I got the recipe from my friend Aditi, but she left it as a voice memo and the speaker on my phone is buzzing so I can't quite make out what she's saying. Could you please listen to the recipe and list all of the ingredients that my friend described? I only want the ingredients for the filling, as I have everything I need to make my favorite pie crust. I've attached the recipe as Strawberry pie.mp3.\n",
            "\n",
            "In your response, please only list the ingredients, not any measurements. So if the recipe calls for \"a pinch of salt\" or \"two cups of ripe strawberries\" the ingredients on the list would be \"salt\" and \"ripe strawberries\".\n",
            "\n",
            "Please format your response as a comma separated list of ingredients. Also, please alphabetize the ingredients.\n",
            "File: /content/drive/MyDrive/GAIA/99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n",
            "Extension detected: mp3\n",
            "Extension detected: mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[92m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
            "\u001b[1m\u001b[96mUSER\u001b[0m: Question: Hi, I'm making a pie but I could use some help with my shopping list. I have everything I need for the crust, but I'm not sure about the filling. I got the recipe from my friend Aditi, but she left it as a voice memo and the speaker on my phone is buzzing so I can't quite make out what she's saying. Could you please listen to the recipe and list all of the ingredients that my friend described? I only want the ingredients for the filling, as I have everything I need to make my favorite pie crust. I've attached the recipe as Strawberry pie.mp3.\n",
            "\n",
            "In your response, please only list the ingredients, not any measurements. So if the recipe calls for \"a pinch of salt\" or \"two cups of ripe strawberries\" the ingredients on the list would be \"salt\" and \"ripe strawberries\".\n",
            "\n",
            "Please format your response as a comma separated list of ingredients. Also, please alphabetize the ingredients.\n",
            "Filepath: /content/drive/MyDrive/GAIA/99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n",
            "\u001b[1m\u001b[92m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
            "\u001b[1m\u001b[96mget_information_from_audio\u001b[0m: Information extracted from /content/drive/MyDrive/GAIA/99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3.\n",
            "\n",
            "Transcription of audio file:\n",
            "\n",
            " In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.\n",
            "\n",
            "Please verify this information by cross-checking with reliable external sources such as web searches, Wikipedia, or other knowledge bases before finalizing your response.\n",
            "If necessary, supplement the transcription with additional relevant information to ensure completeness and accuracy.\n",
            "\u001b[1m\u001b[92m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
            "\u001b[1m\u001b[96mASSISTANT\u001b[0m: \n",
            "Here's my plan:\n",
            "1. Use the `get_information_from_audio` tool to transcribe the audio file.\n",
            "2. Extract the ingredients from the transcription.\n",
            "3. Alphabetize the ingredients.\n",
            "4. Return the ingredients as a comma-separated list.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[92m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
            "\u001b[1m\u001b[96mget_information_from_audio\u001b[0m: Transcription of audio file:\n",
            "\n",
            " In a saucepan, combine ripe strawberries, granulated sugar, freshly squeezed lemon juice and cornstarch. Cook the mixture over medium heat, stirring constantly until it thickens to a smooth consistency. Remove from heat and stir in a dash of pure vanilla extract. Allow the strawberry pie feeling to cool before using it as a delicious and fruity filling for your pie crust.\n",
            "\n",
            "Please verify this information by cross-checking with reliable external sources such as web searches, Wikipedia, or other knowledge bases before finalizing your response.\n",
            "If necessary, supplement the transcription with additional relevant information to ensure completeness and accuracy.\n",
            "\u001b[1m\u001b[92m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[0m\n",
            "\u001b[1m\u001b[96mASSISTANT\u001b[0m: The transcribed ingredients are: ripe strawberries, granulated sugar, freshly squeezed lemon juice, cornstarch, and pure vanilla extract.\n",
            "\n",
            "Alphabetizing these gives: cornstarch, granulated sugar, freshly squeezed lemon juice, pure vanilla extract, ripe strawberries.\n",
            "\n",
            "FINAL ANSWER: cornstarch, granulated sugar, freshly squeezed lemon juice, pure vanilla extract, ripe strawberries\n",
            "\n",
            "\u001b[1m\u001b[93m======================================================================================================================================================\u001b[0m\n",
            "('cornstarch, granulated sugar, freshly squeezed lemon juice, pure vanilla '\n",
            " 'extract, ripe strawberries')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.draw_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Rea25j8RJQDt",
        "outputId": "0d7cb20d-9618-4c8a-fddf-65bff78652ff"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thank you for your time! 😀\n",
        "<img src=\"https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit4/congrats.png\">"
      ],
      "metadata": {
        "id": "qVpp-bRaHUYK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zhNuQmUxz_Ig",
        "W1JwCkBG0kCM",
        "4lPdKbuIO0tm",
        "qz0woGa9C8OC"
      ],
      "mount_file_id": "1QTCbj_qXPK8yIBKDc8QkUHwjVnFAH_tR",
      "authorship_tag": "ABX9TyPbn2sBZvJeEQj661WnoCxJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a81031f77d646839cf678e683e83b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cf9050c5b114889a13a94c80d993f1b",
              "IPY_MODEL_960c75fe623847c5b874a905da553c07",
              "IPY_MODEL_4888175ef94f46c4ba757a97ce0ea16d"
            ],
            "layout": "IPY_MODEL_fad406a35fea4cafaad4b33457a7e301"
          }
        },
        "6cf9050c5b114889a13a94c80d993f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4257164aeab44fdaf697c9e4d9e2058",
            "placeholder": "​",
            "style": "IPY_MODEL_2470a40a507747d1aeafb1171dbba19f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "960c75fe623847c5b874a905da553c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc20cbbc2e664297b12e247eb5a4c815",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f035e73dc7384ad9b776e5dbb0eed64e",
            "value": 2
          }
        },
        "4888175ef94f46c4ba757a97ce0ea16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7dcdee0b1fe49ef85eeb49be8d14f8c",
            "placeholder": "​",
            "style": "IPY_MODEL_ef5e948e163245eb8bdd2c5433d76f40",
            "value": " 2/2 [00:00&lt;00:00,  4.53it/s]"
          }
        },
        "fad406a35fea4cafaad4b33457a7e301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4257164aeab44fdaf697c9e4d9e2058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2470a40a507747d1aeafb1171dbba19f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc20cbbc2e664297b12e247eb5a4c815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f035e73dc7384ad9b776e5dbb0eed64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7dcdee0b1fe49ef85eeb49be8d14f8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5e948e163245eb8bdd2c5433d76f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}